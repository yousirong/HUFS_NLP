{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chater2_8_One_Hot_Encoding.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwH9EW6xRIOd"
      },
      "source": [
        "**08) 원-핫 인코딩(One-Hot Encoding)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7ZJ4-ytRcJS"
      },
      "source": [
        "1. 원-핫 인코딩(One-Hot Encoding)이란?\n",
        "\n",
        "  원-핫-인코딩 두가지 과정으로 정리\n",
        "    - 각 단어에 고유한 인덱스를 부여 (정수 인코딩)\n",
        "    - 표현하고 싶은 단어의 인덱스의 위치에 1을 부여하고, 다른 단어의 인덱스 위치에 0을 부여\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOWEF4RESuI1"
      },
      "source": [
        "!pip3 install konlpy\n",
        "\n",
        "import konlpy\n",
        "print(\"konlpy : \" + konlpy.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N22175IwRHhx"
      },
      "source": [
        "from konlpy.tag import Okt \n",
        "okt = Okt()\n",
        "\n",
        "text = \"나는 자연어 처리를 배운다\"\n",
        "token=okt.morphs(text) # 문장 token\n",
        "\n",
        "print(text)\n",
        "print()\n",
        "\n",
        "print('문장 Token ---------------------------------')\n",
        "print(token)\n",
        "print()\n",
        "\n",
        "word2index={}\n",
        "for voca in token:\n",
        "  if voca not in word2index.keys():\n",
        "    word2index[voca]=len(word2index)\n",
        "\n",
        "print('정수 인덱싱 --------------------------------')\n",
        "print(word2index)\n",
        "print()  \n",
        "\n",
        "def one_hot_encoding(word, word2index):\n",
        "  one_hot_vector = [0]*(len(word2index))\n",
        "  index=word2index[word]\n",
        "  one_hot_vector[index]=1\n",
        "  return one_hot_vector\n",
        "\n",
        "print('one-hot-vector -----------------------------')\n",
        "print(one_hot_encoding('자연어', word2index))\n",
        "print() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvrWgz0gVY4i"
      },
      "source": [
        "2. 케라스(Keras)를 이용한 원-핫 인코딩(One-Hot Encoding)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLD9v-oiVZp3"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "text = \"나랑 점심 먹으러 갈래 점심 메뉴는 햄버거 갈래 갈래 햄버거 최고야\"\n",
        "\n",
        "print(text)\n",
        "print()\n",
        "\n",
        "t = Tokenizer()\n",
        "t.fit_on_texts([text])\n",
        "\n",
        "print('단어별 정수 인덱싱 --------------------------')\n",
        "print(t.word_index) # 각 단어에 대한 인코딩 결과\n",
        "print()\n",
        "\n",
        "sub_text = \"점심 먹으러 갈래 메뉴는 햄버거 최고야\"\n",
        "\n",
        "print(sub_text)\n",
        "print()\n",
        "\n",
        "encoded = t.texts_to_sequences([sub_text])\n",
        "print('정수 인코딩 ---------------------------------')\n",
        "print(encoded)\n",
        "print()\n",
        "\n",
        "one_hot = to_categorical(encoded)\n",
        "print('원-핫-인코딩 --------------------------------')\n",
        "print(one_hot)\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7aix8EvYEdl"
      },
      "source": [
        "3. 원-핫 인코딩(One-Hot Encoding)의 한계\n",
        "\n",
        "  - 벡터를 저장하기 위해 필요한 공간이 계속 늘어나는 단점 (단어 집합의 크기가 곧 벡터의 차원 수)\n",
        "  - 단어의 유사도를 표현하지 못하는 단점"
      ]
    }
  ]
}