{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chater2_3_Stemming_Lemmatization.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgcvlxmSJbxu"
      },
      "source": [
        "**03) 어간 추출(Stemming) and 표제어 추출(Lemmatization)**  \n",
        "\n",
        "  정규화 기법 중 코퍼스에 있는 단어의 개수를 줄일 수 있는 기법인 표제어 추출(lemmatization)과 어간 추출(stemming)의 개념 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4uklfg4J0k7"
      },
      "source": [
        "1. 표제어 추출(Lemmatization)  \n",
        "\n",
        "   형태소의 두가지 종류\n",
        "     - **어간(stem)** : 단어의 의미를 담고 있는 단어의 핵심 부분\n",
        "     - **접사(affix)** : 단어에 추가적인 의믈ㄹ 주눈 부분"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEpasSaBJRnT"
      },
      "source": [
        "# import nltk\n",
        "# nltk.download('wordnet') # wordnet 리소스\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "n=WordNetLemmatizer()\n",
        "\n",
        "words=['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
        "print([n.lemmatize(w) for w in words])\n",
        "print(\"n.lemmatize('dies') -> \" + n.lemmatize('dies'))\n",
        "print(\"n.lemmatize('dies', 'v') -> \" + n.lemmatize('dies', 'v'))\n",
        "print(\"n.lemmatize('watched', 'v') -> \" + n.lemmatize('watched', 'v'))\n",
        "print(\"n.lemmatize('has', 'v') -> \" + n.lemmatize('has', 'v'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7fZM5xPMjzd"
      },
      "source": [
        "2. 어간 추출(Stemming)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJXAAXj4MqSp"
      },
      "source": [
        "# import nltk\n",
        "# nltk.download('punkt') # punkt 리소스 설치\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "s = PorterStemmer()\n",
        "text = \"This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things--names and heights and soundings--with the single exception of the red crosses and the written notes.\"\n",
        "words = word_tokenize(text)\n",
        "print('word_tokenize ------------------------------')\n",
        "print(words)\n",
        "print()\n",
        "\n",
        "print('stem ---------------------------------------')\n",
        "print([s.stem(w) for w in words])\n",
        "print()\n",
        "\n",
        "words=['formalize', 'allowance', 'electricical']\n",
        "print([s.stem(w) for w in words])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kSPRINMRXCY"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "s=PorterStemmer()\n",
        "words=['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
        "print(words)\n",
        "print()\n",
        "\n",
        "print('PorterStemmer  ------------------------------')\n",
        "print([s.stem(w) for w in words])\n",
        "print()\n",
        "\n",
        "from nltk.stem import LancasterStemmer\n",
        "l=LancasterStemmer()\n",
        "\n",
        "print('LancasterStemmer   --------------------------')\n",
        "print([l.stem(w) for w in words])\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnQ87wrvSv2F"
      },
      "source": [
        "words = ['am', 'the going', 'having']\n",
        "print(words)\n",
        "print()\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "n=WordNetLemmatizer()\n",
        "\n",
        "print('표제어 추출(Lemmatization)   --------------------------')\n",
        "print([n.lemmatize(w, pos='v') for w in words])\n",
        "print()\n",
        "\n",
        "from nltk.stem import LancasterStemmer\n",
        "s=LancasterStemmer()\n",
        "\n",
        "print('어간 추출(Stemming)   ---------------------------------')\n",
        "print([s.stem(w) for w in words])\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51UGb3V1XELr"
      },
      "source": [
        "3. 한국어에서의 어간 추출\n",
        "\n",
        "  (1) 활용(conjugation)   \n",
        "    활용이란 용언의 어간(stem)이 어미(ending)를 가지는 일을 말합니다.  \n",
        "  (2) 규칙 활용  \n",
        "    규칙 활용은 어간이 어미를 취할 때, 어간의 모습이 일정합니다.  \n",
        "  (3) 불규칙 활용  \n",
        "    불규칙 활용은 어간이 어미를 취할 때 어간의 모습이 바뀌거나 취하는 어미가 특수한 어미일 경우를 말합니다."
      ]
    }
  ]
}