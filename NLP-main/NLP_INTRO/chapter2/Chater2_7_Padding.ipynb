{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chater2_7_Padding.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJjRvApXlle5"
      },
      "source": [
        "**07) 패딩(Padding)**\n",
        "\n",
        "  여러 문자의 길이를 임의로 동일하게 맞춰주는 작업"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGVfhc01lfWB"
      },
      "source": [
        "# import nltk\n",
        "# nltk.download('punkt')\n",
        "# nltk.download('stopwords')\n",
        "\n",
        "from nltk.tokenize import sent_tokenize # 구문을 문장 단위로 분리\n",
        "from nltk.tokenize import word_tokenize # 문장을 단어 단위로 분리\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "text = \"A barber is a person. a barber is good person. a barber is huge person. he Knew A Secret! The Secret He Kept is huge secret. Huge secret. His barber kept his word. a barber kept his word. His barber kept his secret. But keeping and keeping such a huge secret to himself was driving the barber crazy. the barber went up a huge mountain.\"\n",
        "print('text ---------------------------------------')\n",
        "print(text)\n",
        "print()\n",
        "\n",
        "# 문장 토큰화\n",
        "token_sent = sent_tokenize(text)\n",
        "print('문장 토큰화 --------------------------------')\n",
        "print(token_sent)\n",
        "print()\n",
        "\n",
        "# 정제와 단어 토큰화\n",
        "vocab = {} # 파이썬의 dictionary 자료형\n",
        "sentences = []\n",
        "stop_words = set(stopwords.words('english')) # 불용어 리스트\n",
        "print('불용어 리스트 -------------------------------')\n",
        "print(stop_words)\n",
        "print()\n",
        "\n",
        "for i in token_sent:\n",
        "  token_word = word_tokenize(i) # 단어 토큰화를 수행합니다\n",
        "  result = []\n",
        "\n",
        "  for word in token_word:\n",
        "    word = word.lower() # 모든 단어를 소문자화하여 단어의 개수를 줄입니다\n",
        "    if word not in stop_words: # 단어 토큰화 된 결과에 대해서 불용어를 제거합니다\n",
        "      if len(word) > 2: # 단어 길이가 2 이하인 경우에 대하여 추가로 단어를 제거합니다\n",
        "        result.append(word)\n",
        "        if word not in vocab:\n",
        "          vocab[word] = 0\n",
        "        vocab[word] += 1  \n",
        "\n",
        "  sentences.append(result)\n",
        "\n",
        "print('문장별 단어 토큰 결과(불용어 / 2글자 이하 제거) -------------')\n",
        "print(sentences)\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iX9SLW8Opi2Q"
      },
      "source": [
        "1. Numpy로 패딩하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrhQSP6emeuF"
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "print(sentences)\n",
        "print()\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences) # fit_on_texts()안에 코퍼스를 입력으로 하면 빈도수를 기준으로 단어 집합을 생성한다.\n",
        "\n",
        "encoded = tokenizer.texts_to_sequences(sentences)\n",
        "\n",
        "print('단어별 정수 인덱스 부여 --------------')\n",
        "print(encoded)\n",
        "print()\n",
        "\n",
        "max_len = max(len(item) for item in encoded)\n",
        "print('가장 긴 문장의 길이 ------------------')\n",
        "print(max_len)\n",
        "print()\n",
        "\n",
        "for item in encoded:  # 각 문장에 대해서\n",
        "  while len(item) < max_len:  # max_len 보다 작으면\n",
        "    item.append(0)\n",
        "\n",
        "print(encoded)\n",
        "print()\n",
        "\n",
        "padded_np = np.array(encoded)\n",
        "print('패딩 결과  ---------------------------')\n",
        "print(padded_np)\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryoS5xrPpluW"
      },
      "source": [
        "2. 케라스 전처리 도구로 패딩하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBQoatEVpqb1"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "encoded = tokenizer.texts_to_sequences(sentences)\n",
        "\n",
        "print('단어별 정수 인덱스 부여 --------------')\n",
        "print(encoded)\n",
        "print()\n",
        "\n",
        "padded = pad_sequences(encoded) # pad_sequences는 기본적으로 앞에서 0 을 채움\n",
        "print('패딩 결과  ---------------------------')\n",
        "print(padded)\n",
        "print()\n",
        "\n",
        "padded = pad_sequences(encoded, padding='post') # 뒤에서 0 을 채움\n",
        "print('패딩 결과 (뒤에서 0 ) ----------------')\n",
        "print(padded)\n",
        "print()\n",
        "\n",
        "print('numpy 결과와 비교 --------------------')\n",
        "print(padded == padded_np)\n",
        "print((padded == padded_np).all())\n",
        "\n",
        "\n",
        "padded = pad_sequences(encoded, padding='post')\n",
        "print('패딩 결과 (뒤에서 0 ) ----------------')\n",
        "print(padded)\n",
        "print()\n",
        "\n",
        "\n",
        "padded = pad_sequences(encoded, padding='post', maxlen=5) # 5 개로 길이 제한\n",
        "print('maxlen 을 5 로 정의 ------------------')\n",
        "print(padded)\n",
        "print()\n",
        "\n",
        "print(tokenizer.word_index)\n",
        "last_value = len(tokenizer.word_index) + 1 # 단어 집합의 크기보다 1 큰 숫자를 사용\n",
        "print(last_value)\n",
        "print()\n",
        "\n",
        "padded = pad_sequences(encoded, padding='post', value=last_value) # 패딩 숫자를 (단어 길이+1) 로 정의\n",
        "print('패딩 숫자를 0이 아닌 숫자 -------------')\n",
        "print(padded)\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}