{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chater2_6_Integer_Encoding.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9AnkBdRVGpv"
      },
      "source": [
        "**06) 정수 인코딩(Integer Encoding)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piUvq7oBVPEo"
      },
      "source": [
        "1. 정수 인코딩(Integer Encoding)\n",
        "\n",
        "  단어를 빈도수 순으로 정렬한 단어 집합을 만들고, 빈도수가 높은 순서대로 차례로 낮은 숫자부터 정수를 부여하는 방법"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmzHtlfPVsIJ"
      },
      "source": [
        "1) dictionary 사용하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "up5wcPTaU989"
      },
      "source": [
        "# import nltk\n",
        "# nltk.download('punkt')\n",
        "# nltk.download('stopwords')\n",
        "\n",
        "from nltk.tokenize import sent_tokenize # 구문을 문장 단위로 분리\n",
        "from nltk.tokenize import word_tokenize # 문장을 단어 단위로 분리\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "text = \"A barber is a person. a barber is good person. a barber is huge person. he Knew A Secret! The Secret He Kept is huge secret. Huge secret. His barber kept his word. a barber kept his word. His barber kept his secret. But keeping and keeping such a huge secret to himself was driving the barber crazy. the barber went up a huge mountain.\"\n",
        "print('text ---------------------------------------')\n",
        "print(text)\n",
        "print()\n",
        "\n",
        "# 문장 토큰화\n",
        "token_sent = sent_tokenize(text)\n",
        "print('문장 토큰화 --------------------------------')\n",
        "print(token_sent)\n",
        "print()\n",
        "\n",
        "# 정제와 단어 토큰화\n",
        "vocab = {} # 파이썬의 dictionary 자료형\n",
        "sentences = []\n",
        "stop_words = set(stopwords.words('english')) # 불용어 리스트\n",
        "print('불용어 리스트 -------------------------------')\n",
        "print(stop_words)\n",
        "print()\n",
        "\n",
        "for i in token_sent:\n",
        "  token_word = word_tokenize(i) # 단어 토큰화를 수행합니다\n",
        "  result = []\n",
        "\n",
        "  for word in token_word:\n",
        "    word = word.lower() # 모든 단어를 소문자화하여 단어의 개수를 줄입니다\n",
        "    if word not in stop_words: # 단어 토큰화 된 결과에 대해서 불용어를 제거합니다\n",
        "      if len(word) > 2: # 단어 길이가 2 이하인 경우에 대하여 추가로 단어를 제거합니다\n",
        "        result.append(word)\n",
        "        if word not in vocab:\n",
        "          vocab[word] = 0\n",
        "        vocab[word] += 1  \n",
        "\n",
        "  sentences.append(result)\n",
        "\n",
        "print('문장별 단어 토큰 결과(불용어 / 2글자 이하 제거) -------------')\n",
        "print(sentences)\n",
        "print()\n",
        "\n",
        "print('단어별 빈도수 -------------------------------')\n",
        "print(vocab)\n",
        "print(vocab[\"barber\"]) # 'barber'라는 단어의 빈도수 출력\n",
        "print()\n",
        "\n",
        "# sorted : 이터러블로부터 새로운 정렬된 리스트를 만듬\n",
        "# items() 는 Dictionary 자료형에서 Key, Value 쌍을 가져옴\n",
        "vocab_sorted = sorted(vocab.items(), key = lambda x:x[1], reverse=True) # Value 데이터를 기준으로 역순 정렬\n",
        "print('단어별 빈도수 높은 순서 정렬 -----------------------')\n",
        "print(vocab_sorted)\n",
        "print()\n",
        "\n",
        "word_to_index = {}\n",
        "i=0\n",
        "for (word, frequency) in vocab_sorted:\n",
        "  print(word, frequency)\n",
        "  if frequency > 1: # 정제(Cleaning) 챕터에서 언급했듯이 빈도수가 적은 단어는 제외한다.\n",
        "    i=i+1\n",
        "    word_to_index[word] = i\n",
        "\n",
        "print()\n",
        "print('단어별 정수 인덱스 부여(빈도수 1 제외) --------------')\n",
        "print(word_to_index)\n",
        "print()\n",
        "\n",
        "vocab_size = 5 # 빈도수 상위 5개 단어만 사용\n",
        "words_frequency = [w for w,c in word_to_index.items() if c >= vocab_size + 1] # 인덱스가 5 초과인 단어 제거\n",
        "print('인덱스 5 초과 단어 ----------------------------------')\n",
        "print(words_frequency)\n",
        "print()\n",
        "\n",
        "for w in words_frequency:\n",
        "  del word_to_index[w] # 해당 단어에 대한 인덱스 정보를 삭제\n",
        "\n",
        "print('빈도수 상위 5개 단어 ---------------------------------')\n",
        "print(word_to_index)\n",
        "print()\n",
        "\n",
        "word_to_index['OOV'] = len(word_to_index) + 1\n",
        "print('Out-Of-Vocabulary 인덱스 추가 -------------------------')\n",
        "print(word_to_index)\n",
        "print()\n",
        "\n",
        "encoded = []\n",
        "for s in sentences:\n",
        "  temp = []\n",
        "  for w in s:\n",
        "    try:\n",
        "      temp.append(word_to_index[w])\n",
        "    except KeyError:\n",
        "      temp.append(word_to_index['OOV'])\n",
        "  encoded.append(temp)\n",
        "\n",
        "print('정수 인코딩 결과 -----------------------------------')\n",
        "print(sentences)\n",
        "print(encoded)\n",
        "print()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57awt9ZwYefU"
      },
      "source": [
        "2) Counter 사용하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZHgdgGEYf7Q"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "print(sentences)\n",
        "print()\n",
        "\n",
        "words = sum(sentences, [])\n",
        "# 위 작업은 words = np.hstack(sentences)로도 수행 가능.\n",
        "print('단어 리스트 ---------------------------------')\n",
        "print(words)\n",
        "print()\n",
        "\n",
        "vocab = Counter(words)\n",
        "print('단어별 빈도수 -------------------------------')\n",
        "print(vocab)\n",
        "print(vocab[\"barber\"]) # 'barber'라는 단어의 빈도수 출력\n",
        "print()\n",
        "\n",
        "vocab_size = 5 # 빈도수 상위 5개 단어만 사용\n",
        "vocab = vocab.most_common(vocab_size)\n",
        "print('빈도수 상위 5개 단어 ---------------------------------')\n",
        "print(vocab)\n",
        "print()\n",
        "\n",
        "word_to_index = {}\n",
        "i=0\n",
        "for (word, frequency) in vocab:\n",
        "  print(word, frequency)\n",
        "  i=i+1\n",
        "  word_to_index[word] = i\n",
        "\n",
        "print()\n",
        "print('단어별 정수 인덱스 부여 --------------')\n",
        "print(word_to_index)\n",
        "print()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQRakrb-aToL"
      },
      "source": [
        "3) NLTK의 FreqDist 사용하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30yxbqLdaYlQ"
      },
      "source": [
        "from nltk import FreqDist\n",
        "import numpy as np\n",
        "\n",
        "print(sentences)\n",
        "print()\n",
        "\n",
        "# np.hstack으로 문장 구분을 제거하여 입력으로 사용 . ex) ['barber', 'person', 'barber', 'good' ... 중략 ...\n",
        "vocab = FreqDist(np.hstack(sentences))\n",
        "print(vocab)\n",
        "print(vocab[\"barber\"]) # 'barber'라는 단어의 빈도수 출력\n",
        "print()\n",
        "\n",
        "vocab_size = 5 # 빈도수 상위 5개 단어만 사용\n",
        "vocab = vocab.most_common(vocab_size)\n",
        "print('빈도수 상위 5개 단어 ---------------------------------')\n",
        "print(vocab)\n",
        "print()\n",
        "\n",
        "word_to_index = {word[0] : index + 1 for index, word in enumerate(vocab)}\n",
        "print('단어별 정수 인덱스 부여 --------------')\n",
        "print(word_to_index)\n",
        "print()\n",
        "\n",
        "print('enumerate ----------------------------')\n",
        "for index, value in enumerate(vocab): # 입력의 순서대로 0 부터 인덱스를 부여함\n",
        "  print(\"value : {}, index : {}\".format(value, index))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAfIskrnbTDm"
      },
      "source": [
        "4) enumerate 이해하기\n",
        "\n",
        "enumerate()는 순서가 있는 자료형(list, set, tuple, dictionary, string)을 입력으로 받아 인덱스를 순차적으로 함께 리턴"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhzxjHYQbT8v"
      },
      "source": [
        "test=['a','b','c','d','e']\n",
        "for index, value in enumerate(test): # 입력의 순서대로 0 부터 인덱스를 부여함\n",
        "  print(\"value : {}, index : {}\".format(value, index))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9vpOpI1cMHz"
      },
      "source": [
        "2. 케라스(Keras)의 텍스트 전처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JW7DVXXycccC"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "print(sentences)\n",
        "print()\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences) # fit_on_texts()안에 코퍼스를 입력으로 하면 빈도수를 기준으로 단어 집합을 생성한다.\n",
        "\n",
        "print('단어별 정수 인덱스 부여 --------------')\n",
        "print(tokenizer.word_index)\n",
        "print()\n",
        "\n",
        "print('단어별 빈도수 -------------------------------')\n",
        "print(tokenizer.word_counts)\n",
        "print(tokenizer.word_counts['barber']) # 'barber'라는 단어의 빈도수 출력\n",
        "print()\n",
        "\n",
        "print('단어별 정수 인덱스 부여 --------------')\n",
        "print(tokenizer.texts_to_sequences(sentences))\n",
        "print()\n",
        "\n",
        "print('[상위 5개 단어] ----------------------')\n",
        "print()\n",
        "\n",
        "vocab_size = 5 # 빈도수 상위 5개 단어만 사용\n",
        "tokenizer = Tokenizer(num_words = vocab_size + 1) # 상위 5개 단어만 사용. num_words 는 숫자를 0부터 카운트\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "print('단어별 정수 인덱스 부여 --------------')\n",
        "print(tokenizer.word_index)\n",
        "print()\n",
        "\n",
        "print('단어별 빈도수 -------------------------------')\n",
        "print(tokenizer.word_counts)\n",
        "print(tokenizer.word_counts['barber']) # 'barber'라는 단어의 빈도수 출력\n",
        "print()\n",
        "\n",
        "print('단어별 정수 인덱스 부여 --------------')\n",
        "print(tokenizer.texts_to_sequences(sentences)) # num_words 는 texts_to_sequences 에서 적용됨\n",
        "print()\n",
        "\n",
        "print('[상위 5개 단어 - 다른 방법] ----------------------')\n",
        "print()\n",
        "\n",
        "tokenizer = Tokenizer() # num_words를 여기서는 지정하지 않은 상태\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "vocab_size = 5 # 빈도수 상위 5개 단어만 사용\n",
        "words_frequency = [w for w,c in tokenizer.word_index.items() if c >= vocab_size + 1] # 인덱스가 5 초과인 단어 제거\n",
        "print('정수 인덱스 5 초과 단어 목록 --------------')\n",
        "print(words_frequency)\n",
        "print()\n",
        "\n",
        "for w in words_frequency:\n",
        "  del tokenizer.word_index[w]  # 해당 단어에 대한 인덱스 정보를 삭제\n",
        "  del tokenizer.word_counts[w] # 해당 단어에 대한 카운트 정보를 삭제\n",
        "\n",
        "print(tokenizer.word_index)\n",
        "print(tokenizer.word_counts)\n",
        "print(tokenizer.texts_to_sequences(sentences))  # 케라스 토크나이저는 기본적으로 OOV 에 대해서 정수로 바꾸는 과정에서 제외됨\n",
        "\n",
        "print('[Keras OOV 처리 방법] ----------------------')\n",
        "print()\n",
        "\n",
        "vocab_size = 5 # 빈도수 상위 5개 단어만 사용\n",
        "tokenizer = Tokenizer(num_words = vocab_size + 2, oov_token= 'OOV')\n",
        "# 빈도수 상위 5개 단어만 사용. 숫자 0 과 OOV 를 고려해서 단어 집합의 크기는 +2\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "print('단어 OOV의 인덱스 : {}'.format(tokenizer.word_index['OOV'])) # 케라스 토크나이저는 기본적으로 'OOV'의 인덱스를 1\n",
        "print()\n",
        "\n",
        "print('단어별 정수 인덱스 부여 --------------')\n",
        "print(tokenizer.texts_to_sequences(sentences)) # 빈도수 상위 5개 단어 2 ~ 6 인덱스 사용. OOV 는 1 인덱스 사용\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}