{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chater2_1_Tokenization.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNIIbMf2HBqG"
      },
      "source": [
        "**01) 토큰화(Tokenization)**  \n",
        "\n",
        "  주어진 코퍼스(corpus)에서 토큰(token)이라 불리는 단위로 나누는 작업을 토큰화(tokenization)라고 부릅니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GEfa4D440zX"
      },
      "source": [
        "2. 토큰화 중 생기는 선택의 순간"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-K5eX-bVp1BI"
      },
      "source": [
        "# import nltk\n",
        "# nltk.download('punkt') # punkt 리소스 설치\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "print('word_tokenize ------------------------------')\n",
        "print(word_tokenize(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"))  \n",
        "print()\n",
        "\n",
        "from nltk.tokenize import WordPunctTokenizer \n",
        "print('WordPunctTokenizer ------------------------------')\n",
        "print(WordPunctTokenizer().tokenize(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"))\n",
        "print()\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence \n",
        "print('text_to_word_sequence ------------------------------')\n",
        "print(text_to_word_sequence(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"))\n",
        "print()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ope8dYRr45NG"
      },
      "source": [
        "3. 토큰화에서 고려해야할 사항  \n",
        "  3) 표준 토큰화 예제  \n",
        "   규칙 1. 하이푼으로 구성된 단어는 하나로 유지한다.  \n",
        "   규칙 2. doesn't 와 같이 아포스트로피(')로 '접어'가 함께하는 단어는 분리해준다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vf2PnsI62ak8"
      },
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "tokenizer=TreebankWordTokenizer()\n",
        "# text=\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"\n",
        "text=\"Starting a home-based restaurant may be an ideal. it doesn't have a food chain or restaurant of their own.\"\n",
        "print('TreebankWordTokenizer ------------------------------')\n",
        "print(tokenizer.tokenize(text))\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pDkN-QG4wjh"
      },
      "source": [
        "4. 문장 토큰화(Sentence Tokenization)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_8S5X5g2pao"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "text=\"His barber kept his word. But keeping such a huge secret to himself was driving him crazy. Finally, the barber went up a mountain and almost to the edge of a cliff. He dug a hole in the midst of some reeds. He looked about, to mae sure no one was near.\"\n",
        "print(text)\n",
        "print(sent_tokenize(text))\n",
        "print()\n",
        "\n",
        "from nltk.tokenize import sent_tokenize\n",
        "text=\"I am actively looking for Ph.D. students. and you are a Ph.D student.\"\n",
        "print(text)\n",
        "print(sent_tokenize(text))\n",
        "print()\n",
        "\n",
        "!pip install kss\n",
        "\n",
        "import kss\n",
        "!pip freeze |grep kss # 버전 1.3.1\n",
        "\n",
        "text = \"딥 러닝 자연어 처리가 재미있기는 합니다. 그런데 문제는 영어보다 한국어로 할 때 너우 어려워요. 농담아니예요. 이제 해보면 알걸요?\"\n",
        "print(text)\n",
        "print(kss.split_sentences(text))\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CN_euONBiik"
      },
      "source": [
        "6. 한국어에서의 토큰화의 어려움.  \n",
        "  한국어 토큰화에서는 형태소(morpheme)란 개념을 반드시 이해해야 합니다. 형태소(morpheme)란 뜻을 가진 가장 작은 말의 단위를 말합니다. 이 형태소에는 두 가지 형태소가 있는데 자립 형태소와 의존 형태소입니다.\n",
        "\n",
        "  **자립 형태소** : 접사, 어미, 조사와 상관없이 자립하여 사용할 수 있는 형태소. 그 자체로 단어가 된다. 체언(명사, 대명사, 수사), 수식언(관형사, 부사), 감탄사 등이 있다.  \n",
        "  \n",
        "  **의존 형태소** : 다른 형태소와 결합하여 사용되는 형태소. 접사, 어미, 조사, 어간를 말한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYv_KcTmB1mS"
      },
      "source": [
        "8. NLTK와 KoNLPy를 이용한 영어, 한국어 토큰화 실습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXQHCC7mB3tm"
      },
      "source": [
        "# nltk.download('averaged_perceptron_tagger') # averaged_perceptron_tagger 리소스 설치\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "text=\"I am actively looking for Ph.D. students. and you are a Ph.D. student.\"\n",
        "print(text)\n",
        "print()\n",
        "print('word_tokenize -------------------------------------')\n",
        "print(word_tokenize(text))\n",
        "print()\n",
        "\n",
        "from nltk.tag import pos_tag\n",
        "x=word_tokenize(text)\n",
        "print('pos_tag --------------------------------------------')\n",
        "print(pos_tag(x))\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3YVDiC8CgnH"
      },
      "source": [
        "# !pip3 install konlpy # konlpy 설치\n",
        "\n",
        "from konlpy.tag import Okt\n",
        "okt=Okt()\n",
        "text = \"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"\n",
        "\n",
        "# morphs : 형태소 추출\n",
        "print('[Okt] morphs ---------------------------------------')\n",
        "print(okt.morphs(text))\n",
        "print()\n",
        "\n",
        "# pos : 품사 태깅(Part-of-speech tagging)\n",
        "print('[Okt] pos -------------------------------------------')\n",
        "print(okt.pos(text))\n",
        "print()\n",
        "\n",
        "# nouns : 명사 추출\n",
        "print('[Okt] nouns -----------------------------------------')\n",
        "print(okt.nouns(text))\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVR-0HncD3u8"
      },
      "source": [
        "from konlpy.tag import Kkma\n",
        "kkma=Kkma()\n",
        "text = \"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"\n",
        "\n",
        "# morphs : 형태소 추출\n",
        "print('[Kkma] morphs --------------------------------------')\n",
        "print(okt.morphs(text))\n",
        "print()\n",
        "\n",
        "# pos : 품사 태깅(Part-of-speech tagging)\n",
        "print('[Kkma] pos ------------------------------------------')\n",
        "print(okt.pos(text))\n",
        "print()\n",
        "\n",
        "# nouns : 명사 추출\n",
        "print('[Kkma] nouns ----------------------------------------')\n",
        "print(okt.nouns(text))\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txS7o_pUEnyf"
      },
      "source": [
        "한글 형태소 분석기 비교\n",
        "  - [한국어 형태소 분석기 성능 비교](https://iostream.tistory.com/144)\n",
        "  - [한글 형태소 분석기 비교](http://www.engear.net/wp/%ED%95%9C%EA%B8%80-%ED%98%95%ED%83%9C%EC%86%8C-%EB%B6%84%EC%84%9D%EA%B8%B0-%EB%B9%84%EA%B5%90/)"
      ]
    }
  ]
}