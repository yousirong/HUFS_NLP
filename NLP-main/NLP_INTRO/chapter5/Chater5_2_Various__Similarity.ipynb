{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chater5_2_Various _Similarity.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RiHmsLvkPAh"
      },
      "source": [
        "**2) 여러가지 유사도 기법**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIQd4M3xkRXi"
      },
      "source": [
        "1. 유클리드 거리(Euclidean distance)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rp2xt7ixj-lx"
      },
      "source": [
        "import pandas as pd  # 데이터프레임 사용을 위해\n",
        "from math import log # IDF 계산을 위해\n",
        "\n",
        "docs = [\n",
        "  '사과 바나나 사과 바나나 사과 좋아요',\n",
        "  '저는 사과 저는 바나나 저는 사과 좋아요',\n",
        "  '저는 바나나 사과 좋아요 저는 바나나 좋아요'\n",
        "] \n",
        "\n",
        "vocab = list(set(w for doc in docs for w in doc.split()))\n",
        "print('정렬 전 --------------------------------------')\n",
        "print(vocab)\n",
        "print()\n",
        "vocab.sort()\n",
        "print('정렬 후 --------------------------------------')\n",
        "print(vocab)\n",
        "print()\n",
        "\n",
        "N = len(docs) # 총 문서의 수\n",
        "print('총 문서 수 ------------------------------------')\n",
        "print(N)\n",
        "print()\n",
        "\n",
        "# tf(d,t) : 특정 문서 d에서의 특정 단어 t의 등장 횟수.\n",
        "def tf(t, d): \n",
        "  return d.count(t)\n",
        "\n",
        "\n",
        "# df(t) : 특정 단어 t가 등장한 문서의 수.  \n",
        "def idf(t):\n",
        "  df = 0\n",
        "  for doc in docs:\n",
        "    df += t in doc\n",
        "  return log(N/(df + 1))\n",
        "\n",
        "# idf(d, t) : df(t)에 반비례하는 수.\n",
        "def tfidf(t, d):\n",
        "  return tf(t,d)*idf(t)\n",
        "\n",
        "\n",
        "result=[]\n",
        "for i in range(N): # 각 문서에 대해서 아래 명령을 수행\n",
        "  result.append([])\n",
        "  d = docs[i]\n",
        "  for j in range(len(vocab)):\n",
        "    t = vocab[j]\n",
        "    result[-1].append(tf(t,d))\n",
        "\n",
        "tf_ = pd.DataFrame(result, columns=vocab)\n",
        "print('DTM ----------------------------------------')    \n",
        "print(tf_)\n",
        "print()\n",
        "print(list(tf_.iloc[0]))\n",
        "print(list(tf_.iloc[1]))\n",
        "print(list(tf_.iloc[2]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQJ8hmMpl1pS"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def dist(x,y):\n",
        "  return np.sqrt(np.sum((x-y)**2))\n",
        "\n",
        "doc1 = np.array(tf_.iloc[0])\n",
        "doc2 = np.array(tf_.iloc[1])\n",
        "doc3 = np.array(tf_.iloc[2])\n",
        "docQ = np.array((1,1,0,1))\n",
        "\n",
        "print(dist(doc1, docQ)) # 문서간의 거리가 가장 가깝다는 것을 의미. 즉, 문서1 이 문서Q 와 가장 유사\n",
        "print(dist(doc2, docQ))\n",
        "print(dist(doc3, docQ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3uPmSgokS3t"
      },
      "source": [
        "2. 자카드 유사도(Jaccard similarity)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nK-DebMsnemk"
      },
      "source": [
        "# 다음과 같은 두 개의 문서가 있습니다.\n",
        "# 두 문서 모두에서 등장한 단어는 apple과 banana 2개.\n",
        "doc1 = \"apple banana everyone like likey watch card holder\"\n",
        "doc2 = \"apple banana coupon passport love you\"\n",
        "\n",
        "# 토근화를 수행합니다\n",
        "tokenized_doc1 = doc1.split()\n",
        "tokenized_doc2 = doc2.split()\n",
        "\n",
        "# 토큰화 결과 출력\n",
        "print(tokenized_doc1)\n",
        "print(tokenized_doc2)\n",
        "print()\n",
        "\n",
        "# 문서1 과 문서2의 합집합\n",
        "union = set(tokenized_doc1).union(set(tokenized_doc2))\n",
        "print('문서1 과 문서2의 합집합 --------------------')  \n",
        "print(union)\n",
        "print(len(union))\n",
        "print()\n",
        "\n",
        "# 문서1 과 문서2의 교집합\n",
        "intersection = set(tokenized_doc1).intersection(set(tokenized_doc2))\n",
        "print('문서1 과 문서2의 교집합 --------------------')  \n",
        "print(intersection)\n",
        "print(len(intersection))\n",
        "print()\n",
        "\n",
        "# 자카드 유사도\n",
        "print('자카드 유사도 ------------------------------') \n",
        "print(len(intersection)/len(union)) # 두 집합의 교집합 / 두 집합의 합집합\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}