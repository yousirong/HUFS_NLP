{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chater6_2_Latent_Dirichlet_Allocation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaPXGroo-Wtg"
      },
      "source": [
        "**2) 잠재 디리클레 할당(Latent Dirichlet Allocation, LDA)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4VG7FwF-ZTh"
      },
      "source": [
        "3. LDA의 수행하기\n",
        "\n",
        "  1. 사용자는 알고리즘에게 토픽의 개수 k를 알려줍니다.\n",
        "  2. 모든 단어를 k개 중 하나의 토픽에 할당합니다.\n",
        "  3. 이제 모든 문서의 모든 단어에 대해서 아래의 사항을 반복 진행합니다. (iterative)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGUtravP_n-A"
      },
      "source": [
        "5. 실습을 통한 이해"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWWBemL59JlA"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers','footers','quotes'))\n",
        "documents = dataset.data\n",
        "print('뉴스그룹 데이터 개수 -------------------------') \n",
        "print(len(documents))\n",
        "print()\n",
        "print('첫번재 훈련용 데이터 -------------------------') \n",
        "print(documents[1])\n",
        "print()\n",
        "print('target_names ---------------------------------') \n",
        "print(dataset.target_names)\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFFVLmb3_y8R"
      },
      "source": [
        " import nltk\n",
        " nltk.download('stopwords')\n",
        "\n",
        "# 1. 텍스트 전처리\n",
        "news_df = pd.DataFrame({'document': documents})\n",
        "\n",
        "# 특수문자 제거\n",
        "news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z]\",\" \")\n",
        "# 길이가 3이하인 단어는 제거 (길이가 짧은 단어 제거)\n",
        "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
        "# 전체 단어에 대한 소문자 변환\n",
        "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())\n",
        "\n",
        "print('원문 -----------------------------------------') \n",
        "print(news_df['document'][1])\n",
        "print()\n",
        "print('전처리 ---------------------------------------') \n",
        "print(news_df['clean_doc'][1])\n",
        "print()\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english') # NLTK 로 부터 불용어를 받아옵니다.\n",
        "tokenized_doc = news_df['clean_doc'].apply(lambda x: x.split()) # 토큰화\n",
        "print('불용어 제거 전 -------------------------------') \n",
        "print(tokenized_doc[1])\n",
        "print()\n",
        "\n",
        "tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words]) # 불용어 제거\n",
        "print('불용어 제거 후 -------------------------------') \n",
        "print(tokenized_doc[1])\n",
        "print()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkDbMdwRB3Zn"
      },
      "source": [
        "1) 정수 인코딩과 단어 집합 만들기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LY-8d-ZMAS_O"
      },
      "source": [
        "print(tokenized_doc[:5])\n",
        "print()\n",
        "\n",
        "from gensim import corpora\n",
        "dictionary = corpora.Dictionary(tokenized_doc)\n",
        "print(len(dictionary))\n",
        "print()\n",
        "\n",
        "corpus = [dictionary.doc2bow(text) for text in tokenized_doc]\n",
        "print(corpus[1]) # 수행된 결과에서 두번째 뉴스 출력, 첫번째 문서의 인덱스는 0\n",
        "print()\n",
        "\n",
        "print(dictionary[66])\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_rMWKB7B465"
      },
      "source": [
        "2) LDA 모델 훈련시키기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X07fJkdcB81V"
      },
      "source": [
        "import gensim\n",
        "\n",
        "NUM_TOPICS = 20 #20개의 토픽, k=20\n",
        "\n",
        "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\n",
        "topics = ldamodel.print_topics(num_words=4)\n",
        "for topic in topics:\n",
        "    print(topic)\n",
        "\n",
        "print()\n",
        "print(ldamodel.print_topics())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m88If1j10SFJ"
      },
      "source": [
        "3) LDA 시각화 하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_bvzfTY0Sry"
      },
      "source": [
        "!pip3 install pyLDAvis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmD3CCcQ05ph"
      },
      "source": [
        "import pyLDAvis.gensim\n",
        "pyLDAvis.enable_notebook()\n",
        "vis = pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)\n",
        "pyLDAvis.display(vis)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dh6Vn_j2UZb"
      },
      "source": [
        "4) 문서 별 토픽 분포 보기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coPLNUoM2U_p"
      },
      "source": [
        "for i, topic_list in enumerate(ldamodel[corpus]):\n",
        "  if i == 5:\n",
        "    break\n",
        "  print(i, '번째 문서의 topic 비율은', topic_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UW8QwEA20QJ"
      },
      "source": [
        " def make_topictable_per_doc(ldamodel, corpus):\n",
        "    topic_table = pd.DataFrame()\n",
        "\n",
        "    # 몇 번째 문서인지를 의미하는 문서 번호와 해당 문서의 토픽 비중을 한 줄씩 꺼내온다.\n",
        "    for i, topic_list in enumerate(ldamodel[corpus]):\n",
        "        doc = topic_list[0] if ldamodel.per_word_topics else topic_list            \n",
        "        doc = sorted(doc, key=lambda x: (x[1]), reverse=True)\n",
        "        # 각 문서에 대해서 비중이 높은 토픽순으로 토픽을 정렬한다.\n",
        "        # Ex) 정렬 전 0번 문서 : (2번 토픽, 48.5%), (8번 토픽, 25%), (10번 토픽, 5%), (12번 토픽, 21.5%), \n",
        "        # Ex) 정렬 후 0번 문서 : (2번 토픽, 48.5%), (8번 토픽, 25%), (12번 토픽, 21.5%), (10번 토픽, 5%)\n",
        "        # 48 > 25 > 21 > 5 순으로 정렬이 된 것.\n",
        "\n",
        "        # 모든 문서에 대해서 각각 아래를 수행\n",
        "        for j, (topic_num, prop_topic) in enumerate(doc): #  몇 번 토픽인지와 비중을 나눠서 저장한다.\n",
        "            if j == 0:  # 정렬을 한 상태이므로 가장 앞에 있는 것이 가장 비중이 높은 토픽\n",
        "                topic_table = topic_table.append(pd.Series([int(topic_num), round(prop_topic,4), topic_list]), ignore_index=True)\n",
        "                # 가장 비중이 높은 토픽과, 가장 비중이 높은 토픽의 비중과, 전체 토픽의 비중을 저장한다.\n",
        "            else:\n",
        "                break\n",
        "    return(topic_table)\n",
        "\n",
        "topictable = make_topictable_per_doc(ldamodel, corpus)\n",
        "topictable = topictable.reset_index() # 문서 번호를 의미하는 열(column)로 사용하기 위해서 인덱스 열을 하나 더 만든다.\n",
        "topictable.columns = ['문서 번호','가장 비중이 높은 토픽','가장 높은 토픽의 비중','각 토픽의 비중']\n",
        "print(len(topictable))\n",
        "print(topictable[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ls_J0g_71af"
      },
      "source": [
        "http://s-space.snu.ac.kr/bitstream/10371/95582/1/22_1_%EB%82%A8%EC%B6%98%ED%98%B8.pdf\n",
        "https://bab2min.tistory.com/568\n",
        "https://annalyzin.wordpress.com/2015/06/21/laymans-explanation-of-topic-modeling-with-lda-2/\n",
        "https://towardsdatascience.com/latent-dirichlet-allocation-15800c852699\n",
        "https://radimrehurek.com/gensim/wiki.html#latent-dirichlet-allocation\n",
        "https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/\n",
        "\n",
        "모델 저장 및 로드 하기 : https://stackabuse.com/python-for-nlp-working-with-the-gensim-library-part-2/\n",
        "\n",
        "전반적으로 참고하기 좋은 글 : https://shichaoji.com/2017/04/25/topicmodeling/\n",
        "\n",
        "동영상 강의 : https://blog.naver.com/chunjein/220946362463\n",
        "\n",
        "뉴스를 가지고 할 수 있는 다양한 일들 : https://www.slideshare.net/koorukuroo/20160813-pycon2016apac"
      ]
    }
  ]
}