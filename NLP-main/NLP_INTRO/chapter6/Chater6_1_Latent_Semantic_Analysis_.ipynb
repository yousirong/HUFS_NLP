{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chater6_1_Latent_Semantic_Analysis .ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4y9J8_sYJDb"
      },
      "source": [
        "**1) 잠재 의미 분석(Latent Semantic Analysis, LSA)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbgl9a34YNY3"
      },
      "source": [
        "3. 잠재 의미 분석(Latent Semantic Analysis, LSA)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOKLoDY6X7ok"
      },
      "source": [
        "import pandas as pd  # 데이터프레임 사용을 위해\n",
        "from math import log # IDF 계산을 위해\n",
        "import numpy as np\n",
        "\n",
        "docs = [\n",
        "  '먹고 싶은 사과',\n",
        "  '먹고 싶은 바나나',\n",
        "  '바나나 길고 노란 바나나',\n",
        "  '저는 과일이 좋아요'\n",
        "] \n",
        "\n",
        "vocab = list(set(w for doc in docs for w in doc.split()))\n",
        "print('정렬 전 --------------------------------------')\n",
        "print(vocab)\n",
        "print()\n",
        "vocab.sort()\n",
        "print('정렬 후 --------------------------------------')\n",
        "print(vocab)\n",
        "print()\n",
        "\n",
        "N = len(docs) # 총 문서의 수\n",
        "print('총 문서 수 ------------------------------------')\n",
        "print(N)\n",
        "print()\n",
        "\n",
        "# tf(d,t) : 특정 문서 d에서의 특정 단어 t의 등장 횟수.\n",
        "def tf(t, d): \n",
        "  return d.count(t)\n",
        "\n",
        "\n",
        "# df(t) : 특정 단어 t가 등장한 문서의 수.  \n",
        "def idf(t):\n",
        "  df = 0\n",
        "  for doc in docs:\n",
        "    df += t in doc\n",
        "  return log(N/(df + 1))\n",
        "\n",
        "# idf(d, t) : df(t)에 반비례하는 수.\n",
        "def tfidf(t, d):\n",
        "  return tf(t,d)*idf(t)\n",
        "\n",
        "\n",
        "result=[]\n",
        "for i in range(N): # 각 문서에 대해서 아래 명령을 수행\n",
        "  result.append([])\n",
        "  d = docs[i]\n",
        "  for j in range(len(vocab)):\n",
        "    t = vocab[j]\n",
        "    result[-1].append(tf(t,d))\n",
        "\n",
        "A = pd.DataFrame(result, columns=vocab)\n",
        "print('DTM ----------------------------------------')    \n",
        "print(A)\n",
        "print(np.shape(A))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFRhBCZAZDZx"
      },
      "source": [
        "# svd - 특이값 분해 (Singular Value Decomposition)\n",
        "# [Python NumPy] 선형대수 함수 (Linear Algebra) - 출처 https://rfriend.tistory.com/380\n",
        "# [선형대수] 특이값 분해 (SVD, Singular Value Decomposition) - 출처 https://rfriend.tistory.com/185\n",
        "# [SVD (SVD와 Latent Factor 모형] - 출처 https://www.fun-coding.org/recommend_basic6.html\n",
        "\n",
        "# numpy.linalg.svd - https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html\n",
        "\n",
        "U, s, VT = np.linalg.svd(A, full_matrices=True)\n",
        "print('U ------------------------------------------') \n",
        "print(U)\n",
        "print()\n",
        "print('s ------------------------------------------') \n",
        "print(s)\n",
        "print()\n",
        "print('VT ------------------------------------------') \n",
        "print(VT)\n",
        "print()\n",
        "\n",
        "print('U (직교행렬) --------------------------------') \n",
        "print(U.round(2)) # 소수점 두번째 자리까지 출력\n",
        "print(np.shape(U))\n",
        "print()\n",
        "print('s ------------------------------------------') \n",
        "print(s.round(2)) # 소수점 두번째 자리까지 출력\n",
        "print(np.shape(s))\n",
        "print()\n",
        "S = np.zeros((4,9)) # 대각 행렬의 크기인 4 x 9 의 임의의 행렬 생성\n",
        "S[:4,:4] = np.diag(s) # 특이값을 대각행렬에 삽입\n",
        "print('S (대각행렬) --------------------------------') \n",
        "print(S.round(2)) # 소수점 두번째 자리까지 출력\n",
        "print(np.shape(S))\n",
        "print()\n",
        "\n",
        "print('VT (전치 행렬) -------------------------------') \n",
        "print(VT.round(2)) # 소수점 두번째 자리까지 출력\n",
        "print(np.shape(VT))\n",
        "print()\n",
        "\n",
        "np.allclose(A, np.dot(np.dot(U,S), VT).round(2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1pu3yeaht6_"
      },
      "source": [
        "S=S[:2,:2]\n",
        "print('S (대각행렬) --------------------------------') \n",
        "print(S.round(2))\n",
        "print()\n",
        "\n",
        "U=U[:,:2]\n",
        "print('U (직교행렬) --------------------------------')\n",
        "print(U.round(2))\n",
        "print()\n",
        "\n",
        "VT=VT[:2,:]\n",
        "print('VT (전치 행렬) -------------------------------') \n",
        "print(VT.round(2))\n",
        "print()\n",
        "\n",
        "A_prime=np.dot(np.dot(U,S), VT)\n",
        "print(A)\n",
        "print(A_prime.round(2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zheshEwjZjn"
      },
      "source": [
        "4. 실습을 통한 이해"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V364REsWlYJU"
      },
      "source": [
        "1) 뉴스그룹 데이터에 대한 이해"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFvKcaKFjeji"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers','footers','quotes'))\n",
        "documents = dataset.data\n",
        "print('뉴스그룹 데이터 개수 -------------------------') \n",
        "print(len(documents))\n",
        "print()\n",
        "print('첫번재 훈련용 데이터 -------------------------') \n",
        "print(documents[1])\n",
        "print()\n",
        "print('target_names ---------------------------------') \n",
        "print(dataset.target_names)\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5T8gDF4JlZ38"
      },
      "source": [
        "2) 텍스트 전처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14fOLiUOlfaZ"
      },
      "source": [
        "#  import nltk\n",
        "#  nltk.download('stopwords')\n",
        "\n",
        "# 1. 텍스트 전처리\n",
        "news_df = pd.DataFrame({'document': documents})\n",
        "\n",
        "# 특수문자 제거\n",
        "news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z]\",\" \")\n",
        "# 길이가 3이하인 단어는 제거 (길이가 짧은 단어 제거)\n",
        "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
        "# 전체 단어에 대한 소문자 변환\n",
        "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())\n",
        "\n",
        "print('원문 -----------------------------------------') \n",
        "print(news_df['document'][1])\n",
        "print()\n",
        "print('전처리 ---------------------------------------') \n",
        "print(news_df['clean_doc'][1])\n",
        "print()\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english') # NLTK 로 부터 불용어를 받아옵니다.\n",
        "tokenized_doc = news_df['clean_doc'].apply(lambda x: x.split()) # 토큰화\n",
        "print('불용어 제거 전 -------------------------------') \n",
        "print(tokenized_doc[1])\n",
        "print()\n",
        "\n",
        "tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words]) # 불용어 제거\n",
        "print('불용어 제거 후 -------------------------------') \n",
        "print(tokenized_doc[1])\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E91BZm9Zlba3"
      },
      "source": [
        "3) TF-IDF 행렬 만들기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzgujfDRkRx2"
      },
      "source": [
        "# 역토큰화 (토큰화 작업을 역으로 되돌림)\n",
        "detokenized_doc = []\n",
        "for i in range(len(news_df)):\n",
        "  t = ' '.join(tokenized_doc[i])\n",
        "  detokenized_doc.append(t)\n",
        "\n",
        "news_df['clean_doc'] = detokenized_doc\n",
        "\n",
        "print('역토큰화 ---------------------------------------') \n",
        "print(news_df['clean_doc'][1])\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mVypRsZzxg6"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(stop_words='english',\n",
        "                             max_features = 1000, # 상위 1000개의 단어를 보존\n",
        "                             max_df = 0.5,\n",
        "                             smooth_idf = True)\n",
        "\n",
        "X = vectorizer.fit_transform(news_df['clean_doc'])\n",
        "print(X.shape) # TF-IDE 행렬의 크기 확인"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWHoI6vrlc9j"
      },
      "source": [
        "4) 토픽 모델링(Topic Modeling)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdKot6tv0ITZ"
      },
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "svd_model = TruncatedSVD(n_components=30, algorithm='randomized', n_iter=100, random_state=122)\n",
        "svd_model.fit(X)\n",
        "print(len(svd_model.components_))\n",
        "print()\n",
        "\n",
        "print(np.shape(svd_model.components_))\n",
        "print()\n",
        "\n",
        "terms = vectorizer.get_feature_names() # 단어 집합. 1,000개의 단어가 저장됨.\n",
        "print(len(terms))\n",
        "print(terms[:10])\n",
        "print()\n",
        "\n",
        "def get_topics(components, feature_names, n=5):\n",
        "  for idx, topic in enumerate(components):\n",
        "    print(\"Topic %d:\" % (idx+1), [(feature_names[i], topic[i].round(5)) for i in topic.argsort()[:-n - 1:-1]])\n",
        "\n",
        "get_topics(svd_model.components_, terms)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MCMIdMy_7ma",
        "outputId": "0be0fabd-2fc5-4ac6-b9b9-a3c7e7c0bce7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "source": [
        "tokenized_doc[:5]"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    [well, sure, story, seem, biased, disagree, st...\n",
              "1    [yeah, expect, people, read, actually, accept,...\n",
              "2    [although, realize, principle, strongest, poin...\n",
              "3    [notwithstanding, legitimate, fuss, proposal, ...\n",
              "4    [well, change, scoring, playoff, pool, unfortu...\n",
              "Name: clean_doc, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    }
  ]
}