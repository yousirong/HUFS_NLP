{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chater6_3_LDA.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60GACrCYfqkW"
      },
      "source": [
        "**3) 잠재 디리클레 할당(LDA) 실습2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kImHXfEofvZa"
      },
      "source": [
        "1) 뉴스 기사 제목 데이터에 대한 이해"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWoDlBrgfe79"
      },
      "source": [
        "import pandas as pd\n",
        "import urllib.request\n",
        "\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/franciscadias/data/master/abcnews-date-text.csv\", filename=\"abcnews-date-text.csv\")\n",
        "data = pd.read_csv('abcnews-date-text.csv', error_bad_lines=False)\n",
        "\n",
        "print(len(data))\n",
        "print()\n",
        "print(data[:5])\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gt1FfF_gMo4"
      },
      "source": [
        "text = data[['headline_text']]\n",
        "text = text.copy() \n",
        "print(text.head(5))\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3o6xvLefwyB"
      },
      "source": [
        "2) 텍스트 전처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxdhLfnygWbN"
      },
      "source": [
        "# 토큰화\n",
        "\n",
        "import nltk\n",
        "# nltk.download('punkt')\n",
        "\n",
        "text['headline_text'] = text.apply(lambda row: nltk.word_tokenize(row['headline_text']), axis=1)\n",
        "\n",
        "print(text.head(5))\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-axOdXOujYKa"
      },
      "source": [
        "# 불용어 제거\n",
        "# nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english') # NLTK 로 부터 불용어를 받아옵니다.\n",
        "text['headline_text'] = text['headline_text'].apply(lambda x: [item for item in x if item not in stop_words]) # 불용어 제거\n",
        "\n",
        "print(text.head(5))\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sbwh7sB5kavV"
      },
      "source": [
        "# 표제어 추출\n",
        "# nltk.download('wordnet')\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "text['headline_text'] = text['headline_text'].apply(lambda x: [WordNetLemmatizer().lemmatize(word, pos='v') for word in x])\n",
        "\n",
        "print(text.head(5))\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ooJo3NF-k62G"
      },
      "source": [
        "# 길이가 3 이하인 단어 제거\n",
        "tokenized_doc = text['headline_text'].apply(lambda x: [word for word in x if len(word) > 3])\n",
        "\n",
        "print(tokenized_doc[:5])\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTd1HGqffyHS"
      },
      "source": [
        "3) TF-IDF 행렬 만들기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8cps_YallfZ"
      },
      "source": [
        "# 역토큰화 (토큰화 작업을 되돌림)\n",
        "detokenized_doc = []\n",
        "for i in range(len(text)):\n",
        "    t = ' '.join(tokenized_doc[i])\n",
        "    detokenized_doc.append(t)\n",
        "\n",
        "text['headline_text'] = detokenized_doc # 다시 text['headline_text']에 재저장\n",
        "\n",
        "print(text['headline_text'][:5])\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-cvjepGnR-C"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=1000) # 상위 1000개의 단어를 보존\n",
        "X= vectorizer.fit_transform(text['headline_text'])\n",
        "print(X.shape) # TF-IDF 행렬의 크기 확인"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Scc4t6wAfzca"
      },
      "source": [
        "4) 토픽 모델링"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axJT3_Z6oA9k"
      },
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "lda_model=LatentDirichletAllocation(n_components=10,learning_method='online',random_state=777,max_iter=1)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VS3PZ-rDpsnz"
      },
      "source": [
        "lda_top=lda_model.fit_transform(X)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0gZDkUqps5Y"
      },
      "source": [
        "print(lda_model.components_)\n",
        "print(lda_model.components_.shape) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2bexNswpvM_"
      },
      "source": [
        "terms = vectorizer.get_feature_names() # 단어 집합. 1,000개의 단어가 저장됨.\n",
        "\n",
        "def get_topics(components, feature_names, n=5):\n",
        "    for idx, topic in enumerate(components):\n",
        "        print(\"Topic %d:\" % (idx+1), [(feature_names[i], topic[i].round(2)) for i in topic.argsort()[:-n - 1:-1]])\n",
        "\n",
        "get_topics(lda_model.components_,terms)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}