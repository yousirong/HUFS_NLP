{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chater4_2_Bag_of_Words.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBr72-LfhKOc"
      },
      "source": [
        "**2) Bag of Words(BoW)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayypawHviPmC"
      },
      "source": [
        "!pip3 install konlpy\n",
        "\n",
        "import konlpy\n",
        "print(\"konlpy : \" + konlpy.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtmjSBgXhUO5"
      },
      "source": [
        "1. Bag of Words란?\n",
        "\n",
        "  - 단어들의 순서는 전혀 고려하지 않고, 단어들의 출현 빈도(frequency)에만 집중하는 텍스트 데이터의 수치화 표현 방법\n",
        "  - Bow 를 만드는 과정\n",
        "    - 우선, 각 단어에 고유한 정수 인덱스를 부여\n",
        "    - 각 인덱스의 위치에 단어 토큰의 등장 횟수를 기록한 벡터 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOwXyPWlhBiw"
      },
      "source": [
        "from konlpy.tag import Okt\n",
        "import re\n",
        "\n",
        "okt = Okt()\n",
        "\n",
        "text = \"정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다.\"\n",
        "\n",
        "# 정규표현식을 통해 온점을 제거하는 정제 작업\n",
        "text=re.sub(\"(\\.)\",\"\",text)  \n",
        "print(text)\n",
        "print()\n",
        "# OKT 형태소 분석기를 통해 토큰화 작업을 수행한 뒤에, token 에다가 넣습니다.\n",
        "token1 = okt.morphs(text)\n",
        "print(token1)\n",
        "print()\n",
        "\n",
        "word2index={}\n",
        "bow1=[]\n",
        "\n",
        "for voca in token1:\n",
        "  if voca not in word2index.keys():\n",
        "    word2index[voca]=len(word2index)  # token을 읽으면서, word2index에 없는 (not in) 단어는 새로 추가하고, 이미 있는 단어는 넘깁니다.\n",
        "    bow1.insert(len(word2index)-1,1)  # BoW 전체에 전부 기본값 1을 넣어줍니다. 단어의 개수는 최소 1개 이상이기 때문입니다.  \n",
        "  else:\n",
        "    index=word2index.get(voca)        # 재등장하는 단어의 인덱스를 받아옵니다.\n",
        "    bow1[index]=bow1[index]+1         # 재등장한 단어는 해당하는 인덱스의 위치에 1을 더해줍니다. (단어의 개수를 세는 것입니다.)  \n",
        "\n",
        "print('word2index -----------------------------------')\n",
        "print(word2index)\n",
        "print()\n",
        "print('bow ------------------------------------------')\n",
        "print(bow1)\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akJ4JBMvhUzx"
      },
      "source": [
        "2. Bag of Words의 다른 예제들"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIeCjWvYkiZa"
      },
      "source": [
        "text = \"소비자는 주로 소비하는 상품을 기준으로 물가상승률을 느낀다.\"\n",
        "\n",
        "# 정규표현식을 통해 온점을 제거하는 정제 작업\n",
        "text=re.sub(\"(\\.)\",\"\",text)  \n",
        "print(text)\n",
        "print()\n",
        "# OKT 형태소 분석기를 통해 토큰화 작업을 수행한 뒤에, token 에다가 넣습니다.\n",
        "token2 = okt.morphs(text)\n",
        "print(token2)\n",
        "print()\n",
        "\n",
        "word2index={}\n",
        "bow2=[]\n",
        "\n",
        "for voca in token2:\n",
        "  if voca not in word2index.keys():\n",
        "    word2index[voca]=len(word2index)  # token을 읽으면서, word2index에 없는 (not in) 단어는 새로 추가하고, 이미 있는 단어는 넘깁니다.\n",
        "    bow2.insert(len(word2index)-1,1)  # BoW 전체에 전부 기본값 1을 넣어줍니다. 단어의 개수는 최소 1개 이상이기 때문입니다.  \n",
        "  else:\n",
        "    index=word2index.get(voca)        # 재등장하는 단어의 인덱스를 받아옵니다.\n",
        "    bow2[index]=bow2[index]+1         # 재등장한 단어는 해당하는 인덱스의 위치에 1을 더해줍니다. (단어의 개수를 세는 것입니다.)  \n",
        "\n",
        "print('word2index -----------------------------------')\n",
        "print(word2index)\n",
        "print()\n",
        "print('bow ------------------------------------------')\n",
        "print(bow2)\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfKrQACUlSpX"
      },
      "source": [
        "text = \"정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다. 소비자는 주로 소비하는 상품을 기준으로 물가상승률을 느낀다.\"\n",
        "\n",
        "# 정규표현식을 통해 온점을 제거하는 정제 작업\n",
        "text=re.sub(\"(\\.)\",\"\",text)  \n",
        "print(text)\n",
        "print()\n",
        "# OKT 형태소 분석기를 통해 토큰화 작업을 수행한 뒤에, token 에다가 넣습니다.\n",
        "token3 = okt.morphs(text)\n",
        "print(token3)\n",
        "print()\n",
        "\n",
        "word2index={}\n",
        "bow3=[]\n",
        "\n",
        "for voca in token3:\n",
        "  if voca not in word2index.keys():\n",
        "    word2index[voca]=len(word2index) # token을 읽으면서, word2index에 없는 (not in) 단어는 새로 추가하고, 이미 있는 단어는 넘깁니다.\n",
        "    bow3.insert(len(word2index)-1,1)  # BoW 전체에 전부 기본값 1을 넣어줍니다. 단어의 개수는 최소 1개 이상이기 때문입니다.  \n",
        "  else:\n",
        "    index=word2index.get(voca)       # 재등장하는 단어의 인덱스를 받아옵니다.\n",
        "    bow3[index]=bow3[index]+1          # 재등장한 단어는 해당하는 인덱스의 위치에 1을 더해줍니다. (단어의 개수를 세는 것입니다.)  \n",
        "\n",
        "print('word2index -----------------------------------')\n",
        "print(word2index)\n",
        "print()\n",
        "print('bow ------------------------------------------')\n",
        "print(bow3)\n",
        "print()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeGis7uUhV5T"
      },
      "source": [
        "3. CountVectorizer 클래스로 BoW 만들기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QR3MHOZLocpV"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "text = 'you know I want your love. because I love you.'\n",
        "print(text)\n",
        "print()\n",
        "\n",
        "corpus = [text]\n",
        "vector = CountVectorizer()\n",
        "print(vector.fit_transform(corpus).toarray()) # 코퍼스로부터 각 단어의 빈도 수를 기록한다.\n",
        "print(vector.vocabulary_) # 각 단어의 인덱스가 어떻게 부여되었는지를 보여준다.\n",
        "print()\n",
        "\n",
        "text = '정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다.'\n",
        "print(text)\n",
        "print()\n",
        "\n",
        "corpus = [text]\n",
        "vector = CountVectorizer()\n",
        "print(vector.fit_transform(corpus).toarray()) # 코퍼스로부터 각 단어의 빈도 수를 기록한다.\n",
        "print(vector.vocabulary_) # 각 단어의 인덱스가 어떻게 부여되었는지를 보여준다.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aJUMVwohZ5y"
      },
      "source": [
        "4. 불용어를 제거한 BoW 만들기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQ8Nk8yRpXAX"
      },
      "source": [
        "# (1) 사용자가 직접 정의한 불용어 사용\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "text = \"Family is not an important thing. It's everything.\"\n",
        "print(text)\n",
        "print()\n",
        "\n",
        "corpus = [text]\n",
        "vector = CountVectorizer(stop_words=[\"the\", \"a\", \"an\", \"is\", \"not\"])\n",
        "print(vector.fit_transform(corpus).toarray()) # 코퍼스로부터 각 단어의 빈도 수를 기록한다.\n",
        "print(vector.vocabulary_) # 각 단어의 인덱스가 어떻게 부여되었는지를 보여준다.\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CimyEgN6pwIb"
      },
      "source": [
        "# (2) CountVectorizer에서 제공하는 자체 불용어 사용\n",
        "\n",
        "# import nltk\n",
        "# nltk.download('stopwords')\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "text = \"Family is not an important thing. It's everything.\"\n",
        "print(text)\n",
        "print()\n",
        "\n",
        "corpus = [text]\n",
        "vector = CountVectorizer(stop_words=\"english\")\n",
        "print(vector.fit_transform(corpus).toarray()) # 코퍼스로부터 각 단어의 빈도 수를 기록한다.\n",
        "print(vector.vocabulary_) # 각 단어의 인덱스가 어떻게 부여되었는지를 보여준다.\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEuEzIsyqO8R"
      },
      "source": [
        "# (3) NLTK에서 지원하는 불용어 사용\n",
        "\n",
        "# import nltk\n",
        "# nltk.download('stopwords')\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "text = \"Family is not an important thing. It's everything.\"\n",
        "print(text)\n",
        "print()\n",
        "\n",
        "corpus = [text]\n",
        "sw = stopwords.words(\"english\")\n",
        "vector = CountVectorizer(stop_words=sw)\n",
        "print(vector.fit_transform(corpus).toarray()) # 코퍼스로부터 각 단어의 빈도 수를 기록한다.\n",
        "print(vector.vocabulary_) # 각 단어의 인덱스가 어떻게 부여되었는지를 보여준다.\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}