{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "자연어처리_실습5_[201904458]_[이준용].ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEgUdN1Vi8TN"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import shutil\n",
        "import os\n",
        "import unicodedata\n",
        "import urllib3\n",
        "import zipfile\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwepGRGTjkLv"
      },
      "source": [
        "# 데이터 로드"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hw4KcKdti_mL"
      },
      "source": [
        "http = urllib3.PoolManager()\n",
        "url ='http://www.manythings.org/anki/fra-eng.zip'\n",
        "filename = 'fra-eng.zip'\n",
        "path = os.getcwd()\n",
        "zipfilename = os.path.join(path, filename)\n",
        "with http.request('GET', url, preload_content=False) as r, open(zipfilename, 'wb') as out_file:       \n",
        "    shutil.copyfileobj(r, out_file)\n",
        "\n",
        "with zipfile.ZipFile(zipfilename, 'r') as zip_ref:\n",
        "    zip_ref.extractall(path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFEx_FQRjpp9"
      },
      "source": [
        "# 총 33,000개의 샘플을 사용할 예정"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baaVJRSqjCBz"
      },
      "source": [
        "num_samples = 33000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjDKKFYnjtkw"
      },
      "source": [
        "# 전처리 함수 구현"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_woKYjOGjE_-"
      },
      "source": [
        "def unicode_to_ascii(s):\n",
        "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "      if unicodedata.category(c) != 'Mn')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPehim28jHSU"
      },
      "source": [
        "def preprocess_sentence(sent):\n",
        "    # 위에서 구현한 함수를 내부적으로 호출\n",
        "    sent = unicode_to_ascii(sent.lower())\n",
        "\n",
        "    # 단어와 구두점 사이에 공백을 만듭니다.\n",
        "    # Ex) \"he is a boy.\" => \"he is a boy .\"\n",
        "    sent = re.sub(r\"([?.!,¿])\", r\" \\1\", sent)\n",
        "\n",
        "    # (a-z, A-Z, \".\", \"?\", \"!\", \",\") 이들을 제외하고는 전부 공백으로 변환합니다.\n",
        "    sent = re.sub(r\"[^a-zA-Z!.?]+\", r\" \", sent)\n",
        "\n",
        "    sent = re.sub(r\"\\s+\", \" \", sent)\n",
        "    return sent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkV-jYOOjxv9"
      },
      "source": [
        "# 구현한 전처리 함수 테스트 : 임의의 문장 입력"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6TjXXsqjJWf"
      },
      "source": [
        "# 전처리 테스트\n",
        "en_sent = u\"Have you had dinner?\"\n",
        "fr_sent = u\"Avez-vous déjà diné?\"\n",
        "print(preprocess_sentence(en_sent))\n",
        "print(preprocess_sentence(fr_sent).encode('utf-8'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bFrtHu2j3xC"
      },
      "source": [
        "샘플 데이터를 불러와 모든 전처리를 수행하는 함수 구현\n",
        "\n",
        "- teacher forcing을 위해 디코더의 입/출력 시퀀스를 따로 분리하여 저장\n",
        "- 입출력 심볼 추가\n",
        "- 총 3개의 데이터 셋 생성: 인코더의 입력, 디코더의 입력, 디코더의 실제값"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJmekEI1jMLV"
      },
      "source": [
        "def load_preprocessed_data():\n",
        "    encoder_input, decoder_input, decoder_target = [], [], []\n",
        "\n",
        "    with open(\"fra.txt\", \"r\") as lines:\n",
        "        for i, line in enumerate(lines):\n",
        "\n",
        "            # source 데이터와 target 데이터 분리\n",
        "            src_line, tar_line, _ = line.strip().split('\\t')\n",
        "\n",
        "            # source 데이터 전처리\n",
        "            src_line_input = [w for w in preprocess_sentence(src_line).split()]\n",
        "\n",
        "            # target 데이터 전처리\n",
        "            tar_line = preprocess_sentence(tar_line)\n",
        "            tar_line_input = [w for w in (\"<sos> \" + tar_line).split()]\n",
        "            tar_line_target = [w for w in (tar_line + \" <eos>\").split()]\n",
        "\n",
        "            encoder_input.append(src_line_input)\n",
        "            decoder_input.append(tar_line_input)\n",
        "            decoder_target.append(tar_line_target)\n",
        "\n",
        "            if i == num_samples - 1:\n",
        "                break\n",
        "\n",
        "    return encoder_input, decoder_input, decoder_target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgn6Uwd3j-5M"
      },
      "source": [
        "# 데이터셋 출력하여 확인"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwsXwdesjQIb"
      },
      "source": [
        "sents_en_in, sents_fra_in, sents_fra_out = load_preprocessed_data()\n",
        "print(sents_en_in[:5])\n",
        "print(sents_fra_in[:5])\n",
        "print(sents_fra_out[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-39cbtFTkYwY"
      },
      "source": [
        "- 단어집합 생성\n",
        "- 정수 인코딩"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JP9JnYiOkec9"
      },
      "source": [
        "tokenizer_en = Tokenizer(filters=\"\", lower=False)\n",
        "tokenizer_en.fit_on_texts(sents_en_in)\n",
        "encoder_input = tokenizer_en.texts_to_sequences(sents_en_in)\n",
        "\n",
        "tokenizer_fra = Tokenizer(filters=\"\", lower=False)\n",
        "tokenizer_fra.fit_on_texts(sents_fra_in)\n",
        "tokenizer_fra.fit_on_texts(sents_fra_out)\n",
        "decoder_input = tokenizer_fra.texts_to_sequences(sents_fra_in)\n",
        "decoder_target = tokenizer_fra.texts_to_sequences(sents_fra_out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jyH-w90krsI"
      },
      "source": [
        "- 패딩"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCbhl3csktdD"
      },
      "source": [
        "encoder_input = pad_sequences(encoder_input, padding=\"post\")\n",
        "decoder_input = pad_sequences(decoder_input, padding=\"post\")\n",
        "decoder_target = pad_sequences(decoder_target, padding=\"post\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R921wioglMKp"
      },
      "source": [
        "- 단어 집합의 크기 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_ttVNhBlOTb"
      },
      "source": [
        "src_vocab_size = len(tokenizer_en.word_index) + 1\n",
        "tar_vocab_size = len(tokenizer_fra.word_index) + 1\n",
        "print(\"영어 단어 집합의 크기 : {:d}, 프랑스어 단어 집합의 크기 : {:d}\".format(src_vocab_size, tar_vocab_size))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JO9Paj83ljjy"
      },
      "source": [
        "- 단어로부터 정수를 얻는 딕셔너리와\n",
        "- 정수로부터 단어를 얻는 딕셔너리를 각각 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6agNfvsloSp"
      },
      "source": [
        "src_to_index = tokenizer_en.word_index\n",
        "index_to_src = tokenizer_en.index_word # 훈련 후 결과 비교할 때 사용\n",
        "\n",
        "tar_to_index = tokenizer_fra.word_index # 훈련 후 예측 과정에서 사용\n",
        "index_to_tar = tokenizer_fra.index_word # 훈련 후 결과 비교할 때 사용"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUxn8o7Dl6i_"
      },
      "source": [
        "테스트 데이터 분리\n",
        "- 적절한 분포를 갖도록 데이터를 섞어 줌"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTFvaZ8pLxPE"
      },
      "source": [
        "indices = np.arange(encoder_input.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "print(indices)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-CF1od8mCIS"
      },
      "source": [
        "# 순서가 섞인 정수 시퀀스 리스트 생성\n",
        "\n",
        "\n",
        "\n",
        "# indices =[]\n",
        "# for encoder_input in range(10000):\n",
        "#     if(encoder_input%100 != 53):\n",
        "#       continue\n",
        "#     elif(encoder_input>=1054):\n",
        "#       break\n",
        "#     else:\n",
        "#       indices = np.append(indices,encoder_input)\n",
        "# indices = np.array(indices)\n",
        "# np.random.shuffle(indices)\n",
        "# print(indices)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0IPizN0mKRy"
      },
      "source": [
        "# 이를 데이터셋의 순서로 지정해주면 샘플들이 기존 순서와 다른 순서로 섞임\n",
        "\n",
        "encoder_input = encoder_input[indices]\n",
        "decoder_input = decoder_input[indices]\n",
        "decoder_target = decoder_target[indices]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7j0vfmHmXIp"
      },
      "source": [
        "# 확인\n",
        "\n",
        "# encoder_input[10053]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00Zdyt8ompWo"
      },
      "source": [
        "# decoder_input[30997]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTbFBraQmreH"
      },
      "source": [
        "# decoder_target[30997]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmNqQMbLm5zM"
      },
      "source": [
        "- 훈련 데이터의 10%를 테스트 데이터로 분리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tgg-pcHBm645"
      },
      "source": [
        "n_of_val = int(33000*0.1)\n",
        "print(n_of_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2OaQrS0nB9U"
      },
      "source": [
        "# 33,000개의 10%에 해당되는 3,300개의 데이터를 테스트 데이터로 사용\n",
        "\n",
        "encoder_input_train = encoder_input[:-n_of_val]\n",
        "decoder_input_train = decoder_input[:-n_of_val]\n",
        "decoder_target_train = decoder_target[:-n_of_val]\n",
        "\n",
        "encoder_input_test = encoder_input[-n_of_val:]\n",
        "decoder_input_test = decoder_input[-n_of_val:]\n",
        "decoder_target_test = decoder_target[-n_of_val:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gj1JmQElnLRR"
      },
      "source": [
        "# 훈련 데이터와 테스트 데이터의 크기(shape)를 출력\n",
        "\n",
        "print(encoder_input_train.shape)\n",
        "print(decoder_input_train.shape)\n",
        "print(decoder_target_train.shape)\n",
        "print(encoder_input_test.shape)\n",
        "print(decoder_input_test.shape)\n",
        "print(decoder_target_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwXnHaPLngFs"
      },
      "source": [
        "2. 기계 번역기 만들기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSQ-CugsnqmY"
      },
      "source": [
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Masking\n",
        "from tensorflow.keras.models import Model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQBEZ3f_nvTV"
      },
      "source": [
        "# 임베딩 벡터와 LSTM의 은닉 상태의 크기를 50으로 고정\n",
        "\n",
        "latent_dim = 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOAAcv_On4aJ"
      },
      "source": [
        "- 모델 설계"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ovhOdIHn54p"
      },
      "source": [
        "# 인코더\n",
        "\n",
        "encoder_inputs = Input(shape=(None,))\n",
        "enc_emb =  Embedding(src_vocab_size, latent_dim)(encoder_inputs) # 임베딩 층\n",
        "enc_masking = Masking(mask_value=0.0)(enc_emb) # 패딩 0은 연산에서 제외\n",
        "encoder_lstm = LSTM(latent_dim, return_state=True) # 상태값 리턴을 위해 return_state는 True\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(enc_masking) # 은닉 상태와 셀 상태를 리턴\n",
        "encoder_states = [state_h, state_c] # 인코더의 은닉 상태와 셀 상태를 저장"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVBPQljSoK5I"
      },
      "source": [
        "# 디코더\n",
        "\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "dec_emb_layer = Embedding(tar_vocab_size, latent_dim) # 임베딩 층\n",
        "dec_emb = dec_emb_layer(decoder_inputs) # 패딩 0은 연산에서 제외\n",
        "dec_masking = Masking(mask_value=0.0)(dec_emb)\n",
        "\n",
        "# 상태값 리턴을 위해 return_state는 True, 모든 시점에 대해서 단어를 예측하기 위해 return_sequences는 True\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True) \n",
        "\n",
        "# 인코더의 은닉 상태를 초기 은닉 상태(initial_state)로 사용\n",
        "decoder_outputs, _, _ = decoder_lstm(dec_masking,\n",
        "                                     initial_state=encoder_states)\n",
        "\n",
        "# 모든 시점의 결과에 대해서 소프트맥스 함수를 사용한 출력층을 통해 단어 예측\n",
        "decoder_dense = Dense(tar_vocab_size, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxO9-qgOoQod"
      },
      "source": [
        "# 모델 정의\n",
        "\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEKHXdQxoqdV"
      },
      "source": [
        "- 훈련 실행"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6K4jdvVos0r"
      },
      "source": [
        "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
        "\n",
        "# 현재 decoder_outputs의 경우에는 원-핫 인코딩을 하지 않은 상태이므로\n",
        "# sparse_categorical_crossentropy를 사용"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDFlardNo5sm"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJ7-LhAPo8P-"
      },
      "source": [
        "model.fit(x = [encoder_input_train, decoder_input_train], y = decoder_target_train, \\\n",
        "          validation_data = ([encoder_input_test, decoder_input_test], decoder_target_test),\n",
        "          batch_size = 128, epochs = 50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdfZiVpVq0iZ"
      },
      "source": [
        "3. seq2seq 기계 번역기 동작시키기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7fW9tYNq2VX"
      },
      "source": [
        "# 테스트를 위해 모델 재설계\n",
        "\n",
        "# 인코더\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "# 디코더\n",
        "# 이전 시점의 상태를 보관할 텐서\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "# 훈련 때 사용했던 임베딩 층을 재사용\n",
        "dec_emb2= dec_emb_layer(decoder_inputs)\n",
        "\n",
        "# 다음 단어 예측을 위해 이전 시점의 상태를 현 시점의 초기 상태로 사용\n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n",
        "decoder_states2 = [state_h2, state_c2]\n",
        "\n",
        "# 모든 시점에 대해서 단어 예측\n",
        "decoder_outputs2 = decoder_dense(decoder_outputs2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKdSApWyrChz"
      },
      "source": [
        "# 디코더 정의\n",
        "\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs2] + decoder_states2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRwd6SsZrTwG"
      },
      "source": [
        "- 테스트 과정에서의 동작을 위한 decode_sequence 함수를 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dn2P1_5erVj0"
      },
      "source": [
        "def decode_sequence(input_seq):\n",
        "    # 입력으로부터 인코더의 상태를 얻음\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # <SOS>에 해당하는 정수 생성\n",
        "    target_seq = np.zeros((1,1))\n",
        "    target_seq[0, 0] = tar_to_index['<sos>']\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "\n",
        "    # stop_condition이 True가 될 때까지 루프 반복\n",
        "    # 구현의 간소화를 위해서 이 함수는 배치 크기를 1로 가정합니다.\n",
        "    while not stop_condition:\n",
        "        # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "        # 예측 결과를 단어로 변환\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = index_to_tar[sampled_token_index]\n",
        "\n",
        "         # 현재 시점의 예측 단어를 예측 문장에 추가\n",
        "        decoded_sentence += ' '+sampled_char\n",
        "\n",
        "        # <eos>에 도달하거나 정해진 길이를 넘으면 중단.\n",
        "        if (sampled_char == '<eos>' or\n",
        "           len(decoded_sentence) > 50):\n",
        "            stop_condition = True\n",
        "\n",
        "        # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
        "        target_seq = np.zeros((1,1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Z9QoVqUrpdH"
      },
      "source": [
        "- 결과 확인을 위한 함수 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TG7XlHnfrqdG"
      },
      "source": [
        "# 원문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
        "def seq2src(input_seq):\n",
        "    temp=''\n",
        "    for i in input_seq:\n",
        "        if(i!=0):\n",
        "            temp = temp + index_to_src[i]+' '\n",
        "    return temp\n",
        "\n",
        "# 번역문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
        "def seq2tar(input_seq):\n",
        "    temp=''\n",
        "    for i in input_seq:\n",
        "        if((i!=0 and i!=tar_to_index['<sos>']) and i!=tar_to_index['<eos>']):\n",
        "            temp = temp + index_to_tar[i] + ' '\n",
        "    return temp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1Oz88YOr_PW"
      },
      "source": [
        "- 훈련 데이터에 대해서 임의로 선택한 인덱스의 샘플의 결과를 출력"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Huq2RcSJsAlm"
      },
      "source": [
        "for seq_index in [158,258,358,458,558,658,658,758,858,958,1058]:\n",
        "  input_seq = encoder_input_train[seq_index: seq_index + 1]\n",
        "  decoded_sentence = decode_sequence(input_seq)\n",
        "\n",
        "  print(\"원문 : \",seq2src(encoder_input_train[seq_index]))\n",
        "  print(\"번역문 :\",seq2tar(decoder_input_train[seq_index]))\n",
        "  print(\"예측문 :\",decoded_sentence[:-5])\n",
        "  print(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3-_YSmjsTn_"
      },
      "source": [
        "- 테스트 데이터에 대해서 임의로 선택한 인덱스의 샘플의 결과를 출력"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtQCAzaQsU6V"
      },
      "source": [
        "for seq_index in [158,258,358,458,558,658,658,758,858,958,1058]:\n",
        "  input_seq = encoder_input_test[seq_index: seq_index + 1]\n",
        "  decoded_sentence = decode_sequence(input_seq)\n",
        "\n",
        "  print(\"원문 : \",seq2src(encoder_input_test[seq_index]))\n",
        "  print(\"번역문 :\",seq2tar(decoder_input_test[seq_index]))\n",
        "  print(\"예측문 :\",decoded_sentence[:-5])\n",
        "  print(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhXnGl1oeb0C"
      },
      "source": [
        "**전체 모델 저장하기**\n",
        "\n",
        "model.save 메서드를 호출하여 모델의 구조, 가중치, 훈련 설정을 하나의 파일/폴더에 저장합니다. 모델을 저장하기 때문에 원본 파이썬 코드*가 없어도 사용할 수 있습니다. 옵티마이저 상태가 복원되므로 정확히 중지한 시점에서 다시 훈련을 시작할 수 있습니다.\n",
        "\n",
        "전체 모델은 두 가지 다른 파일 형식(SavedModel 및 HDF5)으로 저장할 수 있습니다. 전체 모델을 저장하는 기능은 매우 유용합니다. TensorFlow.js로 모델을 로드한 다음 웹 브라우저에서 모델을 훈련하고 실행할 수 있습니다(Saved Model, HDF5). 또는 모바일 장치에 맞도록 변환한 다음 TensorFlow Lite를 사용하여 실행할 수 있습니다(Saved Model, HDF5).\n",
        "\n",
        "**SavedModel 포맷**\n",
        "\n",
        "SavedModel 형식은 모델을 직렬화하는 또 다른 방법입니다. 이 형식으로 저장된 모델은 tf.keras.models.load_model을 사용하여 복원할 수 있으며 TensorFlow Serving과 호환됩니다. \n",
        "\n",
        "\\# 새로운 모델 객체를 만들고 훈련합니다\n",
        "\n",
        "model = create_model()\n",
        "model.fit(train_images, train_labels, epochs=5)\n",
        "\n",
        "\\# SavedModel로 전체 모델을 저장합니다\n",
        "\n",
        "!mkdir -p saved_model\n",
        "model.save('saved_model/my_model')\n",
        "\n",
        "SavedModel 형식은 protobuf 바이너리와 TensorFlow 체크포인트를 포함하는 디렉토리입니다. 저장된 모델 디렉토리를 검사합니다.\n",
        "\n",
        "\n",
        "\\# my_model 디렉토리\n",
        "\n",
        "ls saved_model\n",
        "\n",
        "\\# assests 폴더, saved_model.pb, variables 폴더\n",
        "\n",
        "ls saved_model/my_model\n",
        "\n",
        "저장된 모델로부터 새로운 케라스 모델을 로드합니다:\n",
        "\n",
        "new_model = tf.keras.models.load_model('saved_model/my_model')\n",
        "\n",
        "\\# 모델 구조를 확인합니다\n",
        "\n",
        "new_model.summary()\n",
        "\n",
        "복원된 모델은 원본 모델과 동일한 매개변수로 컴파일되어 있습니다. 이 모델을 평가하고 예측에 사용해 보죠\n",
        "\n",
        "\\# 복원된 모델을 평가합니다\n",
        "\n",
        "loss, acc = new_model.evaluate(test_images,  test_labels, verbose=2)\n",
        "print('복원된 모델의 정확도: {:5.2f}%'.format(100*acc))\n",
        "\n",
        "print(new_model.predict(test_images).shape)\n",
        "\n",
        "**HDF5 파일로 저장하기**\n",
        "\n",
        "\\# 새로운 모델 객체를 만들고 훈련합니다\n",
        "\n",
        "model = create_model()\n",
        "model.fit(train_images, train_labels, epochs=5)\n",
        "\n",
        "\\# 전체 모델을 HDF5 파일로 저장합니다\n",
        "\\# '.h5' 확장자는 이 모델이 HDF5로 저장되었다는 것을 나타냅니다\n",
        "\n",
        "model.save('my_model.h5')\n",
        "\n",
        "\\# 가중치와 옵티마이저를 포함하여 정확히 동일한 모델을 다시 생성합니다\n",
        "\n",
        "new_model = tf.keras.models.load_model('my_model.h5')\n",
        "\n",
        "\\# 모델 구조를 출력합니다\n",
        "new_model.summary()\n",
        "\n",
        "\\# 정확도를 확인해 보겠습니다\n",
        "\n",
        "loss, acc = new_model.evaluate(test_images,  test_labels, verbose=2)\n",
        "print('복원된 모델의 정확도: {:5.2f}%'.format(100*acc))"
      ]
    }
  ]
}