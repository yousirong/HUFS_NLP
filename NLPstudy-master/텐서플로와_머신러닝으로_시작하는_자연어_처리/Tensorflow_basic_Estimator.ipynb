{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tensorflow_basic_Estimator.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7fhrTDAGDRB",
        "colab_type": "text"
      },
      "source": [
        "# Estimator\n",
        "- 모델 구현에만 집중할 수 있는 환경을 제공하는 Tensorflow 고수준의 API\n",
        "- 기본 제공 기능\n",
        "  - 학습(Train) : 정의한 모델 파라미터에 대해 학습\n",
        "  - 평가(Evaluate) : 학습한 모델에 대한 성능을 측정\n",
        "  - 예측(Predict) : 모델을 통해 입력값에 대한 예측값을 받는다.\n",
        "  - 배포(Export) : 사용할 모델을 바이너리 파일로 출력한다.\n",
        "- 기본 제공 모델\n",
        "  - 선형 회귀, 선형 분류, 심층 신경망 분류기, 심층 신경망 회귀 모델 등\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zDVTU9FI7DT",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## 모델 함수 구현 및 학습, 검증, 예측\n",
        "- 모델 함수의 인자는 총 5개이다.\n",
        "  - features : 모델에 적용되는 입력 값. 학습, 검증, 예측 과정 모두에 사용\n",
        "  - labels : 모델의 정답 라벨값을 의미한다.\n",
        "  - mode : 현재 모델 함수가 실행된 모드(학습, 검증, 예측)를 의미한다.\n",
        "  - params : 모델에 적용될 부가적인 하이퍼파라미터 값을 의미한다.\n",
        "  - config : 모델에 적용할 설정값을 의미한다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoNlan2CGGR6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 모델 함수 구현 예시\n",
        "def model_fn(featrues, labels, mode, params, config):\n",
        "  \n",
        "  # 모델 구현 부분\n",
        "  \n",
        "  return tf. estimator.EstimatorSpec(...)\n",
        "\n",
        "tf. estimator.Estimator(model_fn=model_fn, model_dir=...,configs=...,params=...)\n",
        "\n",
        "# 모델 학습 \n",
        "estimator.train()\n",
        "\n",
        "# 학습한 모델 검증\n",
        "estimator.evaluate(input_fn=...)\n",
        "\n",
        "# 학습한 모델을 통한 예측\n",
        "estimator.predict(input_fn=...)\n",
        "\n",
        "def train_input_fn():\n",
        "  \n",
        "  # 데이터 파이프라인 구현 부분 # 데이터의 셔플, 배치, 반복 등의 기능이 들어간다.\n",
        "  \n",
        "  return features, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vjBSZWXSjui",
        "colab_type": "text"
      },
      "source": [
        "## 에스티메이터를 활용한 심층 신경망 분류기 사용 구현\n",
        "- 텍스트의 긍/부정을 예측하는 감정 분석(Sentiment Anlaysis) 모델 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lj9vS0M1S884",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 전처리\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import preprocessing\n",
        "\n",
        "samples=['너 오늘 이뻐 보인다',\n",
        "        '나는 오늘 기분이 더러워',\n",
        "        '끝내주는데, 좋은 일이 있나봐',\n",
        "        '나 좋은 일이 생겼어',\n",
        "        '아 오늘 진짜 짜증나',\n",
        "        '환상적인데, 정말 좋은거 같아']\n",
        "labels = [[1],[0],[1],[1],[0],[1]]\n",
        "\n",
        "tokenizer = preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts(samples)\n",
        "sequences = tokenizer.texts_to_sequences(samples)\n",
        "\n",
        "word_index = tokenizer.word_index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8LwcfrbT_Fv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 데이터 입력 함수 정의\n",
        "EPOCH = 100\n",
        "\n",
        "def train_input_fn():\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((sequences, labels))\n",
        "  dataset = dataset.repeat(EPOCH)\n",
        "  dataset = dataset.batch(1)\n",
        "  dataset = dataset.shuffle(len(sequences))\n",
        "  iterator = dataset.make_one_shot_iterator()\n",
        "  \n",
        "  return iterator.get_next()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nc-9zhpgVMlv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 모델 함수 구현\n",
        "VOCAB_SIZE = len(word_index)+1 # 전체 단어 개수\n",
        "EMB_SIZE = 128 # 임베딩 벡터 사이즈 \n",
        "# Word Embedding은 Word를 R차원의 Vector로 매핑시켜주는 것을 말한다.\n",
        "\n",
        "def model_fn(features,labels,mode):\n",
        "  TRAIN = mode = tf.estimator.ModeKeys.TRAIN\n",
        "  EVAL = mode = tf.estimator.ModeKeys.EVAL\n",
        "  PREDICT = mode = tf.estimator.ModeKeys.PREDICT\n",
        "  \n",
        "  embed_input = tf.keras.layers.Embedding(VOCAB_SIZE, EMB_SIZE)(features)\n",
        "  embed_input = tf.reduce_mean(embed_input, axis=1)\n",
        "  \n",
        "  hidden_layer = tf.keras.layers.Dense(128,activation=tf.nn.relu)(embed_input)\n",
        "  output_layer = tf.keras.layers.Dense(1)(hidden_layer)\n",
        "  output = tf.nn.sigmoid(output_layer)\n",
        "  \n",
        "  loss = tf.losses.mean_squared_error(output,labels) # 손실 함수 : MSE 사용\n",
        "  \n",
        "  if TRAIN:\n",
        "    global_step = tf.train.get_global_step()\n",
        "    train_op = tf.train.AdamOptimizer(1e-3).minimize(loss,global_step) # optimizer : Adam 사용\n",
        "    return tf.estimator.EstimatorSpec(mode = mode, train_op=train_op, loss=loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zS23mq3oYfAw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 565
        },
        "outputId": "03eaca75-a3c7-4472-c029-4479676ec7ae"
      },
      "source": [
        "# 학습\n",
        "DATA_OUT_PATH = './data_out/'\n",
        "\n",
        "import os\n",
        "\n",
        "if not os.path.exists(DATA_OUT_PATH):\n",
        "  os.makedirs(DATA_OUT_PATH)\n",
        "\n",
        "estimator = tf.estimator.Estimator(model_fn=model_fn, model_dir=DATA_OUT_PATH + 'checkpoint/dnn')\n",
        "estimator.train(train_input_fn)\n",
        "estimator.predict(train_input_fn)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_model_dir': './data_out/checkpoint/dnn', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ffaf067e710>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from ./data_out/checkpoint/dnn/model.ckpt-1200\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 1200 into ./data_out/checkpoint/dnn/model.ckpt.\n",
            "INFO:tensorflow:loss = 2.7430315e-05, step = 1200\n",
            "INFO:tensorflow:global_step/sec: 298.36\n",
            "INFO:tensorflow:loss = 8.605086e-06, step = 1300 (0.340 sec)\n",
            "INFO:tensorflow:global_step/sec: 395.849\n",
            "INFO:tensorflow:loss = 9.931052e-06, step = 1400 (0.250 sec)\n",
            "INFO:tensorflow:global_step/sec: 379.501\n",
            "INFO:tensorflow:loss = 8.590405e-06, step = 1500 (0.264 sec)\n",
            "INFO:tensorflow:global_step/sec: 373.844\n",
            "INFO:tensorflow:loss = 1.419101e-05, step = 1600 (0.267 sec)\n",
            "INFO:tensorflow:global_step/sec: 355.465\n",
            "INFO:tensorflow:loss = 1.2688093e-05, step = 1700 (0.284 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 1800 into ./data_out/checkpoint/dnn/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 4.3784753e-06.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<generator object Estimator.predict at 0x7ffaf0d67938>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Z4vr1TCegiO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}