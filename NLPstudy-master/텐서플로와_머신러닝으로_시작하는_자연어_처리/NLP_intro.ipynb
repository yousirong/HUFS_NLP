{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_intro.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jg116907/NLPstudy/blob/master/NLP_intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fm1NyOlOBnb4",
        "colab_type": "text"
      },
      "source": [
        "# 자연어 처리 개요\n",
        "- ### **자연어 처리 핵심 문제**\n",
        "  - 텍스트 분류\n",
        "  - 텍스트 유사도\n",
        "  - 텍스트 생성\n",
        "  - 기계 이해\n",
        "- ### **단어 표현 : 단어 임베딩, 단어 벡터 등으로 표현**\n",
        "  - One-hot encoding\n",
        "    - 단어를 0또는 1로 표현\n",
        "    - 단어 개수에 따라 공간이 무한히 커짐 -> 비효율적\n",
        "    - 단어 벡터의 크기가 너무 크고 값이 희소(sparse)\n",
        "  - 분포 가설\n",
        "    - 벡터의 크기가 작으면서도 단어의 의미를 표현할 수 있는 방법들\n",
        "    - 같은 문맥의 단어, 즉 비슷한 위치에 나오는 단어는 비슷한 의미를 가진다.\n",
        "    1. Count 기반 방법 : SVM(특이값 분해), LSA(잠재의미분석), HAL, Hellinger PCA\n",
        "      - 동시 출현 행렬(Co-occurrence Matrix)를 만들고 이를 변형하는 방식\n",
        "    2. 예측 기반 방법 : Word2vec, NNLM, RNNLM\n",
        "      - 신경망 구조 혹은 모델을 사용해 특정 문맥에서 어떤 단어가 나올 것인지를 예측\n",
        "      - Word2vec\n",
        "      > 1. CBOW(Continuous Bag of Words)\n",
        "          - 어떤 단어를 문맥 안의 주변 단어들을 통해 예측\n",
        "          1. 각 주변 단어들을 One-hot 벡터로 만들어 입력값으로 사용(입력층 벡터)\n",
        "          2. 가중치 행렬을 각 One-hot 벡터에 곱해서 n차원 벡터 생성(n차원 은닉층)\n",
        "          3. 만들어진 n차원 벡터를 모두 더한 후 개수로 나눠 평균 n차원 벡터 생성(출력층 벡터)\n",
        "          4. n차원 벡터에 다시 가중치 행렬을 곱해서 One-hot벡터와 같은 차원의 벡터로 만듦\n",
        "          5. 만들어진 벡터를 실제 예측하려고 하는 단어의 One-hot 벡터와 비교해서 학습\n",
        "        2. Skip-Gram\n",
        "          - 어떤 단어를 가지고 특정 문맥 안의 주변 단어들을 예측\n",
        "          1. 하나의 단어를 One-hot 벡터로 만들어 입력값으로 사용(입력층 벡터)\n",
        "          2. 가중치 행렬을 One-hot 벡터에 곱해서 n차원 벡터 생성(n차원 은닉층)\n",
        "          3. n차원 벡터에 다시 가중치 행렬을 곱해서 One-hot벡터와 같은 차원의 벡터로 만듦\n",
        "          4. 만들어진 벡터를 실제 예측하려는 주변 단어들 각각의 One-hot 벡터와 비교해서 학습\n",
        "          \n",
        "    - Count 기반 방법보다 Word2Vec이 단어 간의 유사도를 훨씬 더 잘 측정, 복잡한 특징도 잘 잡아낸다.\n",
        "    - 만들어진 단어 벡터는 서로에게 유의미한 관계르를 측정할 수 있다.\n",
        "    - 보통은 Skip-Gram이 성능이 더 좋으며, Count 기반고 예측 기반을 합친 Glove라는 단어 표현 방법도 있다.\n",
        "    \n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "- ### **텍스트 분류**\n",
        "  - 자연어 처리 기술을 활용해 특정 텍스트를 사람들이 정한 몇 가지 범주(Class) 중 어느 Class에 속하는지 분류하는 문제\n",
        "  - 2가지 범주 : 이진 분류\n",
        "  - 3가지 이상의 범주 : 다중 범주 분류\n",
        "  - 스팸 분류, 감정 분류 등이 있다.\n",
        "  1. 지도 학습을 통한 텍스트 분류\n",
        "    - Naive Bayes Classifier\n",
        "    - Support Vector Machine\n",
        "    - Neural Network\n",
        "    - Linear Classifier\n",
        "    - Logistic Classifier\n",
        "    - Random Forest\n",
        "  2. 비지도 학습을 통한 텍스트 분류\n",
        "    - k-means Clustering\n",
        "    - Hierachical Clustering\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "- ### **텍스트 유사도**\n",
        "  - 텍스트가 얼마나 유사한지 표현하는 방법 중 하나\n",
        "  - 딥러닝 기반 텍스트 유사도 측정 : 벡터화된 각 문장간의 유사도를 측정\n",
        "    - 유사도 측정 방법 4가지 : 자카드, 유클리디언, 맨하탄, 코사인\n",
        "    1. Jaccard Similarity\n",
        "      - 두 문장을 각각 단어의 집합으로 만든 뒤 집합을 통해 유사도를 측정\n",
        "      - 단어들의 교집합 수 / 단어들의 합집합 수\n",
        "      - 0에서 1사이 값. 1에 가까울 수록 유사도가 높다\n",
        "    2. Cosine Similarity\n",
        "      - 두 벡터값 간의 코사인 각도를 구하는 방법\n",
        "      - -1에서 1사이 값. 1에 가까울 수록 유사도가 높다\n",
        "      - 방향성이 더해지기 때문에 일반적으로 성능이 좋고, 가장 널리 쓰인다.\n",
        "    3. Euclidean Similarity\n",
        "      - 두 점 사이의 최단 거리를 구하는 방법\n",
        "      - 1이상의 값을 가질 수 있기 때문에 Nomalize가 필요\n",
        "    4. Mannhattan Similarity\n",
        "      - 사각형 격자로 이루어진 지도에서 가장 최단 거리를 구하는 방법\n",
        "      - 역시 Nomalize가 필요\n",
        "\n",
        "---\n",
        "\n",
        "- ### **자연어 생성**\n",
        "  - 사람의 대화를 최대한 많이 수집해서 대화를 배우게 하고 지속적으로 평가하는 과정을 반복해서 특정 목적에 맞는 텍스트를 생성하는 것이 주 목적.\n",
        "  - 챗봇, 기계번역 분야\n",
        "  \n",
        "---\n",
        "\n",
        "- ### **기계 이해**\n",
        "  - 기계가 텍스트를 이해하고 논리적 추론을 할 수 있는지 데이터 학습을 통해 보는 것\n",
        "  - 자연어 처리의 개념을 모두 활용해야 한다\n",
        "  - 의미 벡터 추출, 질문과 정보 데이터 간의 유사도 검사, 질의를 기반으로 정보 데이터로 부터 새로운 언어 정보 생성, 정답에 대한 내용을 텍스트 분류로 나누기\n",
        "  - 기계 이해 모델 : 메모리 네트워크\n",
        "  - 기계 이해 데이터셋 : bAbI, SQuAD, VQA(VGGNet + LSTM 사용)\n",
        "  \n",
        "  \n",
        "  "
      ]
    }
  ]
}