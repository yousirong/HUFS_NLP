{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tensorflow_basic_keras.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJlmwDqwabJC",
        "colab_type": "text"
      },
      "source": [
        "# Tensorflow Basic - keras\n",
        "- tf.keras 의 여러가지 기본 module들을 정리"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIfDtl2sajEq",
        "colab_type": "text"
      },
      "source": [
        "## tf.keras.layers.Dense\n",
        "- y = f(Wx + b) 의 기본적인 수식을 만족하는 기본적인 신경망 형태의 층 만들기.\n",
        "- 사용되는 인자 값들은 다음과 같다\n",
        "1. unit : 출력 값의 크기\n",
        "2. activation : 활성화 함수\n",
        "3. use_bias : 편향을 사용할지 여부\n",
        "4. bias_initializer : 편향 초기화 함수\n",
        "5. kernel_regularizer : 가중치 정규화 방법\n",
        "6. bias_regularizer : 편향 정규화 방법\n",
        "7. activity_regularizer : 출력 값 정규화 방법\n",
        "8. kernel_constraint : Optimizer에 의해 업데이트 된 후 가중치에 적용되는 부가적 제약 함수\n",
        "9. bias_constraint : Otimizer에 의해 업데이트 된 후 편향에 적용되는 부가적 제약 함수\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFrZLwsqaArQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdegYRiSaz0v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "899b761f-1ebb-476e-c1d3-42d553c1bb8c"
      },
      "source": [
        "# Dense 층 구성 예시\n",
        "INPUT_SIZE=(20,1)\n",
        "input = tf.placeholder(tf.float32,shape=INPUT_SIZE) # input (20,1)\n",
        "hidden = tf.keras.layers.Dense(units=10,activation=tf.nn.sigmoid)(input) # 은닉층 노드 10개, 활성 함수로 sigmoid 사용 \n",
        "output = tf.keras.layers.Dense(units=2,activation=tf.nn.sigmoid)(hidden) # 최종 출력 노드 2개"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sggmyNBeT-2",
        "colab_type": "text"
      },
      "source": [
        "## tf.keras.layers.Dropout\n",
        "- 과적합 문제를 해결하기 위한 정규화의 대표적인 방법 중 하나\n",
        "- 사용되는 인자 값들은 다음과 같다\n",
        "1. rate : dropout을 적용할 확률.\n",
        "2. noise_shape : 특정 값에만 dropout을 적용\n",
        "3. seed : random 값 고정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FRrkMo7bIgM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dropout 추가\n",
        "INPUT_SIZE=(20,1)\n",
        "input = tf.placeholder(tf.float32,shape=INPUT_SIZE)\n",
        "dropout = tf.keras.layers.Dropout(rate=0.2)(input) # dropout을 노드의 20%에 적용 # tf.nn.dropout의 경우에는 값을 0.2로 줄 경우 80%의 값을 0으로 만든다\n",
        "hidden = tf.keras.layers.Dense(units=10,activation=tf.nn.sigmoid)(input)\n",
        "output = tf.keras.layers.Dense(units=2,activation=tf.nn.sigmoid)(hidden)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3d734vqiJya",
        "colab_type": "text"
      },
      "source": [
        "## tf.keras.Conv1d\n",
        "- 합성곱 연산 중 1-D Array 연산\n",
        "- 자연어 처리 분야에서는 Conv1d, 이미지 처리에서는 Conv2d를 사용\n",
        "- 기본 신경망 Dense와 인자가 비슷하지만 filter 관련 인자가 추가\n",
        "1. filters : 필터의 개수. 출력의 차원수를 나타낸다.\n",
        "2. kernel_size : 필터의 크기. 합성곱이 적용되는 window의 크기\n",
        "3. strides : 적용할 stride의 값 \n",
        "4. padding : padding 방법. \"VALID\" or \"SAME\"\n",
        "5. data_format : 데이터의 표현 방법. \"channel_last\" or \"chanel_first\"\n",
        "6. dilation_rate : dilation 합성곱 사용 시 적용할 dilation 값\n",
        "7. 이하 Dense와 동일...\n",
        "\n",
        "- padding 추가 설명\n",
        "  - 패딩을 사용하면 출력 데이터의 크기를 입력 데이터의 크기와 동일하게 설정할 수 있다. 즉, 입력 데이터의 공간적 크기를 고정한 채로 다음 계층에 전달할 수 있는 것이다. -> \"same\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obVMdKqRiA36",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "INPUT_SIZE=(1,28,28)\n",
        "input = tf.placeholder(tf.float32,shape=INPUT_SIZE)\n",
        "dropout = tf.keras.layers.Dropout(rate=0.2)(input)\n",
        "conv = tf.keras.layers.Conv1D(\n",
        "    filters=10, \n",
        "    kernel_size=3,\n",
        "    padding='same',\n",
        "    activation=tf.nn.relu)(dropout)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCGLghV_oR4-",
        "colab_type": "text"
      },
      "source": [
        "##tf.keras.layers.MaxPool1D\n",
        "- 합성곱 신경망과 함께 쓰이는 기법인 풀링.\n",
        "- 크기를 줄이거나 주요 특징을 추출하기 위해 사용한다.\n",
        "- max-pooling, average-pooling의 두 가지 방법이 있다.\n",
        "- MaxPool1D 는 합성곱과 같이 3D까지 나눠져 있다.\n",
        "- 다음과 같은 인자들이 사용된다.\n",
        "1. pool_size : 풀링을 적용할 필터의 크기.\n",
        "2. strides : 적용할 stride의 값\n",
        "3. padding : \"valid\" or \"same\"\n",
        "4. data_format : \"channel_last\" or \"channel_first\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syZhUAbvoPb1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Conv1D 예제에서 MaxPool1D 추가 해서 최종 output까지 구현\n",
        "INPUT_SIZE=(1,28,28)\n",
        "input = tf.placeholder(tf.float32,shape=INPUT_SIZE)\n",
        "dropout = tf.keras.layers.Dropout(rate=0.2)(input)\n",
        "conv = tf.keras.layers.Conv1D(\n",
        "    filters=10, \n",
        "    kernel_size=3,\n",
        "    padding='same',\n",
        "    activation=tf.nn.relu)(dropout)\n",
        "max_pool = tf.keras.layers.MaxPool1D(pool_size=3, padding='same')(conv)  # max pooling\n",
        "flatten = tf.keras.layers.Flatten()(max_pool) # 완전 연결 계층으로 연결하기 위해서 행렬을 벡터로 전환\n",
        "hidden = tf.keras.layers.Dense(units=50, activation=tf.nn.relu)(flatten)\n",
        "output = tf.keras.layers.Dense(units=10,activation=tf.nn.softmax)(hidden)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}