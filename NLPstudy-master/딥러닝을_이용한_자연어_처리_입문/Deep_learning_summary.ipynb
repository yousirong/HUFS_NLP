{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep_learning_summary.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "fuRhxqhRrSjo",
        "5zz8CnxDYxFT",
        "vHDDEFbaWX83",
        "CrmTCJ00Xo2g",
        "Z33a6UeZdd0l",
        "gRZNRfdkkxVa",
        "R04Sz87xt_Zb"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuRhxqhRrSjo",
        "colab_type": "text"
      },
      "source": [
        "# 인공신경망의 종류\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTLkHgcAro25",
        "colab_type": "text"
      },
      "source": [
        "1. Feed-Foword Neural Network (FFNN) : 순방향 신경망. 입력층에서 출력층 방향으로 전개\n",
        "2. Recurrent Neural Network (RNN) : 순환 신경망. 은닉층의 출력값을 다시 은닉층의 입력으로 사용.\n",
        "3. Fully-connected layer (FC) : Dense layer라고도 함. FFNN이 전결합층으로 이루어져 있으면 fully-connected FFNN이라고 한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zz8CnxDYxFT",
        "colab_type": "text"
      },
      "source": [
        "# 활성화 함수(Activation function)\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkyBPu0DY4AB",
        "colab_type": "text"
      },
      "source": [
        "- 은닉층과 출력층의 뉴런에서 출력값을 결정하는 함수.\n",
        "- 활성 함수는 비선형이어야 한다. -> 선형일 경우 은닉층을 쌓을 때 1회 추가와 여러번 추가의 차이가 없다.\n",
        "- Sigmoid \n",
        "  - Sigmoid함수는 양 끝단에서 기울기가 0에 아주 가까워지기 때문에 기울기의 소실 문제가 발생한다. \n",
        "  - Sigmoid의 은닉층 사용 지양\n",
        "- Hyperbolic tangent function\n",
        "  - 0을 중심으로 하기 때문에 sigmoid 보다는 기울기 소실이 적은 편\n",
        "  - 은닉층에서 sigmoid보다는 많이 사용된다.\n",
        "- ReLU\n",
        "  - 특정 양수값에 수렴하지 않고 연산속도도 빠르다.\n",
        "  - 입력값이 음수면 기울기도 0이 되기 때문에 dying ReLU 문제가 발생\n",
        "- Leaky ReLU\n",
        "  - 입력값이 음수인 경우 0이 아닌 매우 작은 수를 반환\n",
        "- Softmax\n",
        "  - sigmiod와 같이 출력층의 활성 함수로 주로 사용된다.\n",
        "  - sigmoid는 이진 분류의 경우, softmax 다중 클래스 분류의 경우 사용\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHDDEFbaWX83",
        "colab_type": "text"
      },
      "source": [
        "# 손실 함수(Loss function)\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dSN1Fi2WgTU",
        "colab_type": "text"
      },
      "source": [
        "- 실제값과 예측값의 차이를 수치화.\n",
        "- MSE\n",
        "  - 연속형 변수를 예측할 때 사용\n",
        "- Cross-entropy\n",
        "  - 낮은 확률로 예측해서 맞추거나, 높은 확률로 예측해서 맞추는 경우 loss가 더 크다.\n",
        "  - 이진 분류, 다중 클래스 분류에 사용\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrmTCJ00Xo2g",
        "colab_type": "text"
      },
      "source": [
        "# Optimizer\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s191OXKcXsym",
        "colab_type": "text"
      },
      "source": [
        "- batch : 가중치등의 매개 변수의 값을 조정하기 위해 사용하는 데이터의 양. 전체 데이터를 이용할 수도 있다.\n",
        "1. Batch Gradient Descent\n",
        "  - loss를 구할 때 전체 데이터를 고려.\n",
        "  - 한 번의 epoch에 매개변수의 업데이트를 단 한 번 수행한다.\n",
        "  - 시간이 오래걸리고 메모리 사용이 크지만 global minimum을 찾을 수 있다.\n",
        "2. Stochastic Gradient Descent\n",
        "  - 매개변수 값 조정시 전체 데이터가 아닌 임의로 선택된 하나의 데이터에 대해서만 계산\n",
        "  - 정확도가 조금 낮을 수 있으며, 매개변수 변경폭이 불안정하지만 속도가 빠르다.\n",
        "3. Mini-Batch Gradient Descent\n",
        "  - 정해진 양에 대해서만 계산하여 매개변수 값을 조정하는 경사 하강법.\n",
        "  - BGD 보다 빠르고, SGD 보다 안정적이다.\n",
        "4. Momentum\n",
        "  - SGD 에 관성의 개념을 적용\n",
        "  - 계산된 접선의 기울기에 한 시점 전의 접선의 기울기를 일정 비율만큼 반영\n",
        "  - 이 방식으로 local minimum에서 탈출 가능하다.\n",
        "5. Adagrad\n",
        "  - 매개변수마다 lerning rate를 다르게 적용시킨다.(매개변수마다 의미하는 바가 다르기 때문)\n",
        "  - 변화가 많은 매개변수는 학습률이 작게, 변화가 적은 매개변수는 학습률을 높게 설정.\n",
        "6. RMSprop\n",
        "  - Adagrad는 학습을 계속하면 학습률이 지나치게 떨어지는 문제가 있다.\n",
        "  - 수식 개선으로 해당 문제를 개선\n",
        "7. Adam\n",
        "  - RMSprop과 Momentum을 결합한 방법.\n",
        "  - 방향과 학습률 두 가지 모두 개선한 방법이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z33a6UeZdd0l",
        "colab_type": "text"
      },
      "source": [
        "# 학습 방법\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UM3QhvM-qVL9",
        "colab_type": "text"
      },
      "source": [
        "- Epoch\n",
        "  - 전체 데이터에 대해서 순전파와 역전파가 끝난 상태.\n",
        "  - 학습 횟수\n",
        "- Batch size\n",
        "   - batch의 수는 iteration이라고 한다. - > 한 번의 epoch을 끝내기 위해 필요한 batch의 수\n",
        "   - 2000개 데이터에서 200개 샘플 단위로 학습한다면 batch size가 200, iteration이 10이다\n",
        "   - SGD의 경우 batch size가 1이므로 모든 iteration마다 데이터를 선택하여 경사하강법을 수행합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRZNRfdkkxVa",
        "colab_type": "text"
      },
      "source": [
        "# 과적합을 막는 방법들\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hb-aHccroxZG",
        "colab_type": "text"
      },
      "source": [
        "1. 데이터의 양을 늘리기\n",
        "  - 양이 적을 경우 모델이 데이터의 특정 패턴이나 노이즈를 암기하기 쉽다.\n",
        "  - 의도적으로 데이터를 증식,증강(Augmentation)하기도 한다. -> 이미지에서 많이 사용\n",
        "2. 모델의 복잡도를 줄이기\n",
        "  - 모델의 복잡도는 은닉층의 수나 매개변수의 수로 정해진다.\n",
        "  - 모델에 있는 매개변수의 수를 수용력(capacity)라고 표현하기도 한다.\n",
        "3. 가중치 규제(Regularization)를 적용\n",
        "  - 모델의 복잡도를 줄이기 위함\n",
        "    1. L1 규제 : 가중치 w들의 절대값 합계를 비용 함수에 추가\n",
        "    2. L2 규제 : 모든 가중치 w들의 제곱합을 비용 함수에 추가\n",
        "  - 이 두 식은 비용 함수를 최소화하기 위해서 가중치 w들의 값이 작아져야 하는 특성을 가진다. \n",
        "  - 결과적으로 어떤 특성들은 모델에 거의 사용되지 않게된다.\n",
        "4. Dropout\n",
        "  - 학습과정에 신경망의 일부를 사용하지 않는 것. dropout을 0.5로 설정하면 학습 과정마다 랜덤으로 절반의 뉴런을 사용하지 않는다.\n",
        "  - 학습 시에만 사용하고 예측시에는 사용하지 않는다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R04Sz87xt_Zb",
        "colab_type": "text"
      },
      "source": [
        "# Gradient Vanishing, Exploding 방지\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeHXD2760GOp",
        "colab_type": "text"
      },
      "source": [
        "1. Leaky ReLU 사용 - Vanishing 방지\n",
        "2. Gradient Clipping - Exloding 방지\n",
        "3. Weight initialization\n",
        "  - Xavier initialization - Sigmoid, Hyperbolic tagent 사용시 효율적\n",
        "  - He initialization - ReLU 사용시 효율적\n",
        "4. Batch Normalize\n",
        "  - Internal Covariate Shift : 내부 공변량 변화. 층 별로 입력 데이터 분포가 달라지는 현상\n",
        "  - 입력에 대해 평균을 0으로 만들고 정규화. \n",
        "  - 정규화된 데이터에 대해서 scale과 shift를 수행.\n",
        "  - 배치 정규화는 학습 시 배치 단위의 평균과 분산을 차례대로 받아 이동 평균과 이동 분산을 저장해 놓았다가 테스트 할 때 저장해놓았던 평균과 분산으로 정규화한다.\n",
        "5. Layer Nomalization\n",
        "  - 배치 정규화가 feature 마다의 평균과 분산을 구한다면, 층 정규화는 샘플마다의 평균과 분산을 구한다.\n",
        "  - 배치 정규화보다 RNN에 적용하기 수월함."
      ]
    }
  ]
}