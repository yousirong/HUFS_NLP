{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Count_based_word_representation.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "u6NlgdpqXyzf",
        "9lD59nCcb7Tb",
        "SKY5ws4XdxAj",
        "IthJmCCTfgTq"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6NlgdpqXyzf",
        "colab_type": "text"
      },
      "source": [
        "# Bag of Words(BoW)\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSrUZEx2X5uB",
        "colab_type": "text"
      },
      "source": [
        "- 순서가 상관 없음\n",
        "- 문서 간의 유사도를 구하는 작업에 주로 사용\n",
        "  - 예) \"미분\",\"방정식\" 이라는 단어의 빈도수가 높으면 수학관련 문서\n",
        "- 구현 방법\n",
        "  1. 각 단어에 고유한 정수 인덱스를 부여\n",
        "  2. 각 인덱스의 위치에 단어 토큰의 등장 횟수를 기록한 벡터를 만든다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JvSlfKsbzwK",
        "colab_type": "text"
      },
      "source": [
        "## KoNLPY로 BoW 만들기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiax4EBIV9_h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install konlpy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beVQNWw0YcxO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from konlpy.tag import Okt\n",
        "import re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXvjx9xIYiUs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "room = [\"바닥은 삐걱거리는 나무판자로 되어있다.\",\\\n",
        "        \"바닥은 차갑다\",\\\n",
        "        \"방 가운데에는 돌로 된 탁자가 하나 놓여있다.\",\\\n",
        "        \"방에 문이 하나 있다.\",\\\n",
        "        \"문은 나무로 되어있다.\",\\\n",
        "        \"문은 열리지 않는다.\",\\\n",
        "        \"문에는 잠금장치가 걸려있다.\",\\\n",
        "        \"방에 침대가 하나 있다.\",\\\n",
        "        \"탁자 옆에는 의자가 두 개 놓여있다.\",\\\n",
        "        \"침대 밑에는 열쇠가 있다.\",\\\n",
        "        \"방에 창문이 없다.\"]\n",
        "okt = Okt()\n",
        "token = re.sub(\"(\\.)\",\"\",\" \".join(room)) # 정규 표현식을 이용한 온점 제거\n",
        "token = okt.morphs(token) # 토큰화 작업"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAi8UrTQZX6g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "563cbebc-7b0e-425a-ee21-cd64f6837d6d"
      },
      "source": [
        "word2index = {}\n",
        "bow = []\n",
        "for voca in token:\n",
        "  if voca not in word2index.keys():\n",
        "    word2index[voca] = len(word2index)\n",
        "    bow.insert(len(word2index)-1,1)\n",
        "  else:\n",
        "    index = word2index.get(voca)\n",
        "    bow[index] += 1\n",
        "print(word2index)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'바닥': 0, '은': 1, '삐걱': 2, '거리': 3, '는': 4, '나무': 5, '판자': 6, '로': 7, '되어있다': 8, '차갑다': 9, '방': 10, '가운데': 11, '에는': 12, '돌': 13, '된': 14, '탁자': 15, '가': 16, '하나': 17, '놓여있다': 18, '에': 19, '문': 20, '이': 21, '있다': 22, '열': 23, '리지': 24, '않는다': 25, '잠금장치': 26, '걸려있다': 27, '침대': 28, '옆': 29, '의자': 30, '두': 31, '개': 32, '밑': 33, '열쇠': 34, '창문': 35, '없다': 36}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVJScysNa0qC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6b3ef20e-17f0-4324-b003-f470e694a1a2"
      },
      "source": [
        "print(bow)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2, 4, 1, 1, 1, 2, 1, 3, 2, 1, 4, 1, 4, 1, 1, 2, 5, 3, 2, 3, 4, 2, 3, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lD59nCcb7Tb",
        "colab_type": "text"
      },
      "source": [
        "## CountVectorizer로 BoW 만들기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LFuXQaCcNvJ",
        "colab_type": "text"
      },
      "source": [
        "- 영어만 가능\n",
        "- 한국어는 조사 등의 이유로 제대로 된 BoW가 만들어지지 않는다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mqdw7y9b_iA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "3b9f729b-ad47-453e-d64c-c59e225ae2fb"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "corpus = [\"you know I want your love. Because I love you\"]\n",
        "vector = CountVectorizer()\n",
        "print(vector.fit_transform(corpus).toarray())\n",
        "print(vector.vocabulary_) # 기본적으로 길이가 2 이상인 문자만 토큰으로 인식"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 1 2 1 2 1]]\n",
            "{'you': 4, 'know': 1, 'want': 3, 'your': 5, 'love': 2, 'because': 0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKY5ws4XdxAj",
        "colab_type": "text"
      },
      "source": [
        "## 불용어를 제거한 BoW"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfqeuF8Vdz-h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "bd855aaa-59e6-4813-b0ea-5eb70b64d362"
      },
      "source": [
        "# countvectorizer에서 제공하는 불용어 사용\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "text=[\"Family is not an important thing. It's everything.\"]\n",
        "v = CountVectorizer(stop_words=\"english\") # 직접 정의할 수도 있다.\n",
        "print(v.fit_transform(text).toarray())\n",
        "print(v.vocabulary_)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 1 1]]\n",
            "{'family': 0, 'important': 1, 'thing': 2}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8s5fEbHKfRjL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# NLTK에서 제공하는 불용어 사용\n",
        "import nltk\n",
        "nltk.download(\"stopwords\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEyhJjazeiAv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "ea075d0a-64ca-48d0-ce61-da4e5cb951b1"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "sw = stopwords.words(\"english\")\n",
        "v = CountVectorizer(stop_words=sw)\n",
        "print(v.fit_transform(text).toarray())\n",
        "print(v.vocabulary_)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 1 1 1]]\n",
            "{'family': 1, 'important': 2, 'thing': 3, 'everything': 0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IthJmCCTfgTq",
        "colab_type": "text"
      },
      "source": [
        "# 문서 단어 행렬(Document-Term Matrix, DTM)\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6hFJXDsfv0y",
        "colab_type": "text"
      },
      "source": [
        "- 서로 다른 문서들의 BoW를 결합.\n",
        "- 문서들 간의 비교가 가능해진다.\n",
        "- 행과 열을 바꿔서 TDM으로 사용하기 한다.\n",
        "- 한계\n",
        "  1. 희소 표현(Sparse representation)\n",
        "    - 대부분의 문서 벡터 값이 0인 비효율을 보여준다.\n",
        "  2. 단순 빈도 수 기반 접근\n",
        "    - 불용어 또한 높은 빈도수를 가질 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ersLydIFfnM8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "room = [\"바닥은 삐걱거리는 나무판자로 되어있다.\",\\\n",
        "        \"바닥은 차갑다\",\\\n",
        "        \"방 가운데에는 돌로 된 탁자가 하나 놓여있다.\",\\\n",
        "        \"방에 문이 하나 있다.\",\\\n",
        "        \"문은 나무로 되어있다.\",\\\n",
        "        \"문은 열리지 않는다.\",\\\n",
        "        \"문에는 잠금장치가 걸려있다.\",\\\n",
        "        \"방에 침대가 하나 있다.\",\\\n",
        "        \"탁자 옆에는 의자가 두 개 놓여있다.\",\\\n",
        "        \"침대 밑에는 열쇠가 있다.\",\\\n",
        "        \"방에 창문이 없다.\",\\\n",
        "        \"방 안에 또 다른 방이 있다.\",\\\n",
        "        \"방과 방이 창문으로 연결되어 있다.\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egz6y4nqg_xD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from konlpy.tag import Kkma \n",
        "kma = Kkma()\n",
        "tk_list=[re.sub(\"(\\.)\",\"\",sen) for sen in room]\n",
        "tk_list = [kma.morphs(sen) for sen in tk_list]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CybZm80rgEkW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def BoW(tk):\n",
        "  bow = [0 for _ in range(len(word2index))] # 앞 서 만들었던 전체 문서 word2index 활용\n",
        "  for voca in tk:\n",
        "    if voca in word2index.keys():\n",
        "      index = word2index.get(voca)\n",
        "      bow[index] += 1\n",
        "  return bow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PpNa-UHiUi2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "outputId": "e09da031-9b58-4cca-ea1e-3f8bf3e09276"
      },
      "source": [
        "dtm = []\n",
        "for tk in tk_list:\n",
        "  dtm.append(BoW(tk))\n",
        "for d in dtm:\n",
        "  print(d)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdBVWvLsmtLx",
        "colab_type": "text"
      },
      "source": [
        "# TF-IDF(Term Frequency-Inverse Document Frequency)\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPgwX1_tnEG2",
        "colab_type": "text"
      },
      "source": [
        "- 단어의 빈도와 역 문서 빈도를 사용하여 DTM내의 각 단어들마다 중요한 정도를 가중치로 주는 방식.\n",
        "- 문서를 d, 단어를 t, 빈도를 f 라고 가정\n",
        "> - tf(d,t)\n",
        "    - 특정 문서 d에서 특정 단어 t의 등장 횟수.\n",
        "    - DTM에서 각 단어들이 가진 값\n",
        "  - df(t)\n",
        "    - 특정 단어 t가 등장한 문서의 수.\n",
        "    - 문서 내의 t 빈도수와 상관 없이, t가 등장한 문서의 수만 카운트한다.\n",
        "  - idf(t)\n",
        "    - df(t)에 반비례하는 수\n",
        "    $$ idf(d,f) = log({{n} \\over {1+df(t)}}) $$\n",
        "    - df를 바로 역수 취하지 않는 이유는 총 문서의 수 n이 커질수록 idf의 값이 기하급수적으로 커지기 때문이다.\n",
        "- TF-IDF는 문서 전체적으로 자주 등장하는 단어는 중요도가 낮다고 판단.(the,a)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkcPp8CemwFl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "6c8ace38-c035-4ca7-ce28-002d69ab75f6"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "corpus = [\n",
        "    'you know I want your love',\n",
        "    'I like you',\n",
        "    'what should I do ',    \n",
        "]\n",
        "tfidfv = TfidfVectorizer().fit(corpus)\n",
        "print(tfidfv.transform(corpus).toarray())\n",
        "print(tfidfv.vocabulary_)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.         0.46735098 0.         0.46735098 0.         0.46735098\n",
            "  0.         0.35543247 0.46735098]\n",
            " [0.         0.         0.79596054 0.         0.         0.\n",
            "  0.         0.60534851 0.        ]\n",
            " [0.57735027 0.         0.         0.         0.57735027 0.\n",
            "  0.57735027 0.         0.        ]]\n",
            "{'you': 7, 'know': 1, 'want': 5, 'your': 8, 'love': 3, 'like': 2, 'what': 6, 'should': 4, 'do': 0}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}