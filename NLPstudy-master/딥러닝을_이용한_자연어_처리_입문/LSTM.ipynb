{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgorvygMwxfx",
        "colab_type": "text"
      },
      "source": [
        "** 내용 추가 필요"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quEzB5o-ceQF",
        "colab_type": "text"
      },
      "source": [
        "# 바닐라 RNN의 단점\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSWPZ0AGckDE",
        "colab_type": "text"
      },
      "source": [
        "- 비교적 짧은 시퀀스에만 효과적\n",
        "- 시점이 길어질 때 전반부에 위치한 정보의 영향력이 손실된다.\n",
        "  - 장기 의존성 문제(the problem of Long-Term Dependencies)\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlR_dBJvQAS7",
        "colab_type": "text"
      },
      "source": [
        "# LSTM(Long Short-Term Memory)\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFYTLiQYiKMw",
        "colab_type": "text"
      },
      "source": [
        "- LSTM은 은닉층의 메모리 셀에 입력 게이트, 망각 게이트, 출력 게이트를 추가하여 불필요한 기억을 지우고, 기억해야 할 것들을 정한다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwJ9wYbpjDKo",
        "colab_type": "text"
      },
      "source": [
        "# 게이트 순환 유닛(Gated Recurrent Unit, GRU)\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_8XCk09jOGv",
        "colab_type": "text"
      },
      "source": [
        "- LSTM의 장기 의존성 문제 해결책을 유지하면서, 은닉 상태를 업데이트하는 계산을 줄임.\n",
        "- 복잡한 LSTM의 구조를 간단화 시킴.\n",
        "- 업데이트 게이트와 리셋 게이트 두 개만 가진다.\n",
        "```\n",
        "model.add(GRU(hidden_size,input_shape=(timesteps,input_dim)))\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPAbbrSMkTX4",
        "colab_type": "text"
      },
      "source": [
        "# RNN 언어 모델(Recurrent Neural Network Language Model, RNNLM)\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3P0oDK7lkh-L",
        "colab_type": "text"
      },
      "source": [
        "- n-gram과 NNLM은 고정된 개수의 단어만을 입력으로 받아야한다는 단점이 있다.\n",
        "- timestep이라는 개념이 도입된 RNN으로 모델을 만들면 입력의 길이를 고정할 필요가 없다.\n",
        "- RNN 훈련 기법\n",
        "  - 교사 강요(teacher forcing) : 모델이 t 시점에서 예측한 값을 t+1 시점에서 입력으로 사용하지 않고, t시점의 레이블(실제 정답)을 t+1시점의 입력값으로 사용한다."
      ]
    }
  ]
}